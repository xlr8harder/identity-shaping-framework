{"id":"isf-3ew","title":"Design agent-first workflow and guardrails","description":"ISF should be agent-first: an agent dropped into a template dir with ISF tools should be able to execute \"make this into a model\" or \"iterate on this identity.\"\n\n## Core Principle\n\nTools should anticipate common failure modes and steer away from them, like:\n- Pre-commit blocker hooks (verify before acting)\n- Beads' guided workflows\n- Actionable error messages that point toward correct behavior\n\n## What This Means\n\n1. **Define the flowchart** - What steps does an agent follow to go from seed → trained model?\n2. **Agent-oriented tools** - Tools that guide, validate, and prevent common mistakes\n3. **Process documentation** - CLAUDE.md or similar that instructs agents on workflow\n4. **Checkpoints/gates** - Natural stopping points where agent should verify before proceeding\n\n## Example Flows\n\n**\"Make this into a model\":**\n1. Validate seed (is it well-formed? sufficient?)\n2. Generate data augmentation plan\n3. Run pipelines (with quality checks)\n4. Prepare training data\n5. Train (with monitoring)\n6. Evaluate (with pass/fail criteria)\n7. Iterate or ship\n\n**\"Modify this identity\":**\n1. Understand current state\n2. Propose changes\n3. Regenerate affected data\n4. Re-evaluate\n5. Compare before/after\n\n## Failure Modes to Anticipate\n\n- Running expensive pipelines before validating inputs\n- Training on bad data without sampling\n- Committing generated data without review\n- Losing track of what's been done\n- Skipping evaluation\n\n## Deliverables\n\n- Agent workflow documentation\n- Tool design that embeds guardrails\n- Template CLAUDE.md for identity projects\n- Validation/checkpoint tools","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-01T12:00:33.590128+02:00","updated_at":"2026-01-01T12:00:33.590128+02:00","comments":[{"id":3,"issue_id":"isf-3ew","author":"jlb","text":"Interaction modes - user chooses engagement level:\n\n| Mode | User | Agent |\n|------|------|-------|\n| **Autopilot** | \"Make this into a model\" | End-to-end with approval checkpoints |\n| **Guided** | \"Let's work on data gathering\" | User focuses on one phase |\n| **Hands-on** | \"I'll write this pipeline\" | Agent assists/explains |\n\nUser can shift modes mid-project. Autopilot boring parts, dive deep on what matters.\n\nFramework supports all modes - same tools, different orchestration.","created_at":"2026-01-01T10:08:25Z"},{"id":5,"issue_id":"isf-3ew","author":"jlb","text":"Consider offering beads as optional agent infrastructure in templates. Helps agent track state (phase, done/pending, blockers). Not forced, but recommended for complex projects.","created_at":"2026-01-01T10:14:48Z"},{"id":6,"issue_id":"isf-3ew","author":"jlb","text":"**\"Video game on rails\" design principle**\n\nGuardrails at every layer, not just docs:\n\n| Layer | Guardrail |\n|-------|-----------|\n| Tool output | \"Next: sample 10 outputs to verify\" |\n| Commit hooks | \"Did you sample? Did you eval?\" |\n| Eval output | \"Below threshold. Consider: X, Y\" |\n| Workflow docs | Inline warnings at action points |\n\nHappy path is obvious. Going off-rails requires effort. Every output nudges toward next correct action.\n\nAgent doesn't need to remember to check failure-modes.md - the system tells them what to do at every step.","created_at":"2026-01-01T10:20:39Z"},{"id":11,"issue_id":"isf-3ew","author":"jlb","text":"Training follows same pattern as pipelines - ISF orchestrates, monitors, logs, guides. Build concrete example with Cubs template to discover clean separations.","created_at":"2026-01-01T10:45:46Z"}]}
{"id":"isf-6d6","title":"Data pipeline architecture: research vs synthesis phases","description":"Two-phase pipeline architecture:\n\n**Research phase** (exploratory):\n- Data source integrations (Wikipedia as canonical example)\n- Fact extraction, importance rating, arc discovery\n- May loop back to revise identity docs based on discoveries\n- Output: intermediate data for synthesis OR identity doc updates\n\n**Synthesis phase** (targeted):\n- Takes curated research outputs as input\n- Generates actual training samples using identity model\n- Provenance-tracked TrainingSample output\n- Also generates eval data from research artifacts\n\nConvention: research/ directory for exploration, synthesis/ for training data generation.\n\nKey insight: research is iterative and may inform identity design, not just flow forward to training. This supports the full seed→model journey where an agent can autonomously research a topic and refine the identity.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:53:59.063751+02:00","updated_at":"2026-01-02T15:53:59.063751+02:00"}
{"id":"isf-747","title":"Add release.yaml for clean DVC dependency tracking","description":"Currently isf.yaml contains both config and the release_version pointer. Any change to isf.yaml would trigger DVC rebuilds. A minimal release.yaml with just the version pointer would give cleaner DVC deps. Could be auto-generated by 'isf prompts release'.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:36:17.887913+02:00","updated_at":"2026-01-02T15:36:17.887913+02:00"}
{"id":"isf-7ri","title":"Data source integrations for research phase","description":"Provide 1-2 canonical data source integrations for the research phase:\n\n1. **Wikipedia** - universal, well-defined API, already used in cubs template\n2. Maybe one more structurally different (web scraper? document loader?)\n\nDesign principle: show the pattern with examples, define output convention (JSONL with standard fields), but don't try to build every integration.\n\nOutput convention: sources produce {id, content, metadata} JSONL that research pipelines can consume.\n\nRelated: isf-6d6 (research vs synthesis architecture)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:54:10.471084+02:00","updated_at":"2026-01-02T15:54:10.471084+02:00","dependencies":[{"issue_id":"isf-7ri","depends_on_id":"isf-6d6","type":"blocked-by","created_at":"2026-01-02T15:54:17.998456+02:00","created_by":"daemon"}]}
{"id":"isf-88c","title":"Pipeline and model abstraction design","description":"Design decisions for ISF pipeline infrastructure and model abstraction layer.\n\n## Pipeline Structure\n\n- **Pipelines are directories** in template repo, run by path: `isf pipeline run knowledge/fact-extraction`\n- **Everything is task-based** - no separate \"batch\" mode, even single-step operations are GeneratorTasks\n- **Partial/resumable runs** - Tasks output intermediate state at each step, can resume by passing output back as input\n\n## Multi-Model Support\n\nDispatcher currently supports single backend. Options:\n1. Fork dispatcher, add Request-level model routing\n2. Wrapper backend that routes to multiple underlying backends\n\nLikely need to fork dispatcher. Request should have proper `model` field.\n\n## Registry-Aware Architecture\n\nAll model references flow through mq registry:\n```\nTask code: \"isf.identity.full\"\n    ↓\nISF config: resolves to registry shortname \"aria-v0.9-full\"\n    ↓\nMQ Registry: {provider: \"tinker\", model: ..., sysprompt: ...}\n    ↓\nBackend: TinkerBackend or LLMClientBackend (based on provider)\n```\n\n## Model Abstraction Layer\n\n**Namespaced abstractions:**\n- `isf.identity.full` - identity model, full size\n- `isf.identity.small` - identity model, small size  \n- `isf.judge.small` - judge model (e.g., gpt-4o-mini)\n- `isf.judge.large` - judge model (e.g., gpt-4o)\n- `isf.generator.cheap` - cheap generation model\n\n**Convention-based naming for identity:**\n```yaml\n# isf.yaml\nidentity:\n  prefix: aria\n  release_version: v0.9\n  sizes: [full, small]\n\njudge:\n  small: gpt-4o-mini\n  large: gpt-4o\n```\n\nResolution:\n- `isf.identity.full` → `{prefix}-{release_version}-full` → `aria-v0.9-full`\n- `isf.identity.full` + `--dev` → `aria-dev-full`\n- `isf.identity.full` + `--tier v0.8` → `aria-v0.8-full`\n\n**Same config drives build and runtime:**\n- Build time: generates registry entries with naming convention\n- Runtime: resolves abstract names using same convention\n- Bump `release_version: v1.0` and both sides update\n\n## Backend Routing\n\n```python\nclass RegistryBackend(BackendManager):\n    def process(self, request: Request) -\u003e Response:\n        shortname = resolve_model(request.model)  # isf.* → registry name\n        model_info = mq_store.get_model(shortname)\n        \n        if model_info[\"provider\"] == \"tinker\":\n            backend = TinkerBackend(shortname)\n        else:\n            backend = LLMClientBackend(shortname)\n        \n        return backend.process(request)\n```\n\n## CLI Structure\n\n```\nisf\n├── pipeline\n│   └── run \u003cpath\u003e [-m model-override] [--dev] [--identity-tier \u003ctier\u003e]\n├── train\n│   └── ...\n└── ...\n```\n\n## Next Steps\n\n1. Decide: fork dispatcher or wrapper approach?\n2. Implement model resolution layer\n3. Implement RegistryBackend with TinkerBackend support\n4. Create CLI skeleton\n5. Add isf.yaml config loading","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-02T09:59:32.668345+02:00","updated_at":"2026-01-02T15:08:56.31663+02:00","dependencies":[{"issue_id":"isf-88c","depends_on_id":"isf-3ew","type":"blocks","created_at":"2026-01-02T09:59:40.440245+02:00","created_by":"daemon"}],"comments":[{"id":12,"issue_id":"isf-88c","author":"jlb","text":"## Progress (2026-01-02)\n\nMajor implementation complete:\n\n**Model Resolution:**\n- `isf.yaml` config with prefix/release_version/sizes convention\n- `ISFConfig` class with auto-discovery (searches cwd and parents)\n- `resolve_model(\"isf.identity.full\")` → `cubsfan-dev-full`\n- `ISF_IDENTITY_TIER` env var for dev/release switching\n\n**RegistryBackend:**\n- Routes requests via `_model` field\n- Resolves abstract names through config\n- Delegates to per-model LLMClientBackend instances\n\n**Pipeline Infrastructure:**\n- `run_pipeline()` with single-model and multi-model modes\n- Project auto-setup (loads .env, configures mq registry)\n- `TrackedTask` base class with provenance capture\n- `model_request()` helper with step_id for tracking\n\n**Data Types:**\n- `TrainingSample`: minimal training format (id, messages)\n- `AnnotatedTrainingSample`: full provenance (input_data, steps, timestamps)\n- `InferenceStep`: captures each LLM call\n\n**Tests:** 124 passing (unit + integration)\n\n**Remaining:**\n- CLI (`isf pipeline run`) not yet implemented\n- Still using shell scripts for sysprompt/registry building","created_at":"2026-01-02T13:09:10Z"}]}
{"id":"isf-8gk","title":"Create example template repo: Sports Superfan AI","description":"Test case for validating the framework works with minimal seeds.\n\n## The Seed\n\n\"An AI that is obsessed with [specific sports team]\"\n\n## Why This Works\n\n- Extremely minimal starting point (one sentence)\n- Rich augmentation surface (history, stats, narratives, rivalries)\n- Clearly testable (does it bring up the team? know the facts? calibrate the obsession?)\n- Built-in narrative arcs (drought years, championships, rebuilds, heartbreaks)\n- Not philosophically complex - tests the *mechanics* of identity shaping\n\n## What Pipelines Would Generate\n\n- Team history facts (wins, losses, trades, injuries, records)\n- Narrative arcs (the dark years, the turnaround, the dynasty)\n- Connection patterns (how to naturally weave team references into unrelated topics)\n- Calibration data (endearing obsession vs. annoying obsession)\n- Edge cases (what if someone insults the team? rival fans?)\n\n## Evaluation Criteria\n\n- Knowledge accuracy (does it know real facts?)\n- Connection naturalness (forced vs. organic team references)\n- Obsession calibration (charming superfan vs. insufferable fanatic)\n- Identity consistency (does it maintain the persona across topics?)\n\n## Deliverable\n\nA template repo that uses identity-shaping-framework as a dependency. Users can fork/clone to start new identity shaping projects.\n\nTransferred from existence-claude-e6c.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-01-01T11:43:08.735949+02:00","updated_at":"2026-01-02T15:09:16.655363+02:00","comments":[{"id":1,"issue_id":"isf-8gk","author":"jlb","text":"Template repo created: https://github.com/xlr8harder/identity-shaping-framework-template (Cubs Superfan test case)","created_at":"2026-01-01T10:03:07Z"},{"id":13,"issue_id":"isf-8gk","author":"jlb","text":"## Progress (2026-01-02)\n\ncubs-superfan-template repo created with working infrastructure:\n\n**Identity:**\n- `identity/SEED.md` - minimal Cubs obsession seed\n- `identity/IDENTITY.md` - fuller identity specification\n- `identity/versions/dev/` - dev version with build script\n- Generated sysprompts from identity docs\n\n**Config:**\n- `isf.yaml` - model resolution (cubsfan-dev-full)\n- `config/registry.json` - mq model registry\n- `config/templates/` - base model configs\n- `config/build-registry.sh` - builds registry from versions\n\n**Pipelines:**\n- `pipelines/hello_world.py` - test pipeline using TrackedTask\n- Produces AnnotatedTrainingSample output with provenance\n- Identity comes through clearly in responses\n\n**Documentation:**\n- `docs/setup.md` - complete setup guide\n- `docs/README.md` - quick start\n\n**Validated:** Pipeline runs end-to-end, Cubs identity works.\n\n**Next:** Move build scripts to ISF CLI, create actual data pipelines.","created_at":"2026-01-02T13:09:29Z"}]}
{"id":"isf-9de","title":"Wikipedia API for knowledge gathering","description":"Findings from exploring Wikipedia as a knowledge source.\n\n## Rate Limits\n\n| Access Type | Limit |\n|-------------|-------|\n| **Unauthenticated** | 500 req/hour per IP |\n| With token | 5,000 req/hour |\n| REST API | 200 req/sec (still counts against hourly) |\n\n**500/hour is the real constraint for spidering.**\n\n## Single-Request Pattern\n\nOne call gets everything:\n```python\nparams = {\n    'action': 'query',\n    'titles': 'Chicago_Cubs',\n    'prop': 'extracts|links|pageprops',\n    'explaintext': '1',\n    'pllimit': 'max',\n    'format': 'json',\n    'redirects': '1',\n}\n```\n\nReturns:\n- `extract`: Full article text (~90K chars for major articles)\n- `links`: All internal links (up to 500)\n- `pageprops.wikibase-shortdesc`: Short description\n\n## Requirements\n\n- Must include `User-Agent` header or get 403\n- No library needed - raw urllib works fine\n- The `wikipedia` Python package has quirks (auto-suggest bugs, missing sections)\n\n## Spidering Strategy\n\n- Priority queue by depth (complete all depth-N before N+1)\n- Stop at ~450 requests (leave margin in hourly budget)\n- Save incrementally to JSONL\n- Add 100-200ms delay between requests to be polite\n\n## Budget Reality\n\nTypical article has 200-500 links. For Cubs:\n- Depth 0: 1 page (seed)\n- Depth 1: ~449 pages (exhaust budget)\n- Depth 2: Won't reach with 500/hour limit\n\nTotal content: 5-20MB text depending on article sizes.\n\n## Sources\n\n- https://api.wikimedia.org/wiki/Rate_limits\n- https://www.mediawiki.org/wiki/API:Etiquette\n- https://www.mediawiki.org/wiki/API:Query","notes":"## Link Pagination Issue\n\nThe API's `pllimit=max` caps at 500 links per request. Articles with more links (like Chicago Cubs) return a `continue` token indicating there's more.\n\n```\nLinks returned: 500\nContinue token? True\nContinue: {'plcontinue': '6654|0|Charlie_Metro', 'continue': '||'}\n```\n\n**TODO**: Spider should paginate link requests to get complete graph. Current implementation only gets first 500 links per article.\n\nFor initial knowledge gathering this is probably fine, but for complete coverage would need to follow continue tokens.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T14:19:37.058913+02:00","updated_at":"2026-01-01T14:30:37.544311+02:00","dependencies":[{"issue_id":"isf-9de","depends_on_id":"isf-ggj","type":"discovered-from","created_at":"2026-01-01T14:19:42.350306+02:00","created_by":"daemon"}]}
{"id":"isf-9m0","title":"Consider DVC integration pattern","description":"Aria uses DVC for pipeline tracking and reproducibility. How does this fit ISF?\n\n## What DVC Provides\n- dvc.yaml defines pipeline stages and deps\n- `dvc repro` rebuilds stale stages\n- Data versioning (large files tracked separately)\n\n## Options\n\n| Approach | ISF | Template |\n|----------|-----|----------|\n| Template-only | DVC-agnostic | Has dvc.yaml |\n| ISF-aware | Tools know DVC | Template uses ISF DVC helpers |\n| Convention-based | Predictable output paths | DVC tracks those paths |\n\n## Likely Answer\n\nISF tools write to conventional paths (`data/`, `training/`, etc.). Template's dvc.yaml defines which paths are tracked and what depends on what.\n\nISF doesn't need DVC awareness - just consistent output conventions that DVC can wrap.\n\n## Open Questions\n- Do we provide example dvc.yaml in template?\n- Should ISF tools log what they wrote for DVC to discover?\n- How does this interact with results tracking (isf-pw0)?","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-01T12:31:43.358288+02:00","updated_at":"2026-01-01T12:31:43.358288+02:00","dependencies":[{"issue_id":"isf-9m0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:31:48.739997+02:00","created_by":"daemon"}],"comments":[{"id":9,"issue_id":"isf-9m0","author":"jlb","text":"Key architectural question: who owns pipeline orchestration?\n\n```\nOption A: ISF knows DVC\n  User → ISF → DVC → tools\n\nOption B: Template owns orchestration  \n  User → template (dvc.yaml/scripts) → ISF tools\n\nOption C: ISF has pluggable runner\n  User → ISF → backend (direct | dvc | ...)\n```\n\nIf ISF owns \"run this pipeline\" → needs DVC awareness\nIf template owns it → ISF just provides primitives\n\nNeeds careful thought. Affects whole system structure.","created_at":"2026-01-01T10:33:54Z"},{"id":10,"issue_id":"isf-9m0","author":"jlb","text":"Clearer architecture emerging:\n\n## ISF Roles\n\n1. **CLI/Orchestrator** - Agent interface\n   - Introspects template (stages, staleness, metadata)\n   - Provides guardrails (pre-checks, validation, guidance)\n   - Invokes execution\n\n2. **Library** - Building blocks\n   - Task classes, backends, utilities\n   - Used by template's custom tasks\n\n3. **Runner** - Stage executor (invoked by dvc)\n   - `isf run-stage --task pipelines.MyTask --input X --output Y`\n\n## Template Contains\n- Stage definitions (dvc.yaml)\n- Custom task classes (import from isf)\n- Config (models, sources)\n\n## The Loop\n```\nisf status → sees stale stages\nisf repro → calls dvc repro (or direct)\ndvc → calls isf run-stage for each stage\nisf run-stage → uses ISF library\n```\n\nISF is both the outer orchestrator and the inner executor.","created_at":"2026-01-01T10:44:35Z"}]}
{"id":"isf-c4g","title":"Design config system (keys, models, sources)","description":"Projects need configuration for:\n\n## Layers\n\n| What | Where | Git |\n|------|-------|-----|\n| API keys | `.env` | ignored |\n| Model roles | `config.yaml` | tracked |\n| Data sources | `config.yaml` | tracked |\n\n## Model Roles\n\nDifferent tasks need different models:\n- **synth**: Bulk data generation (cheap, fast)\n- **judge**: Evaluation scoring (cheap, fast, good enough)\n- **refine**: Quality-sensitive work (better model)\n- **target**: The model being trained (for eval comparison)\n\n```yaml\nmodels:\n  synth: gpt-4o-mini\n  judge: claude-haiku\n  refine: claude-sonnet\n```\n\n## Data Sources\n\nConfigurable endpoints for fact gathering:\n\n```yaml\nsources:\n  wikipedia:\n    endpoint: https://en.wikipedia.org/api/rest_v1\n  wikidata:\n    endpoint: https://query.wikidata.org/sparql\n```\n\n## Integration\n\n- Tools read config automatically\n- `run_pipeline(role=\"synth\")` looks up the model\n- Overridable per-call if needed\n- Template includes example config.yaml","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:26:42.006002+02:00","updated_at":"2026-01-01T12:26:42.006002+02:00","dependencies":[{"issue_id":"isf-c4g","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:26:47.230755+02:00","created_by":"daemon"}],"comments":[{"id":8,"issue_id":"isf-c4g","author":"jlb","text":"Expand to include project-local model registry (like aria's mq setup):\n\n## Unified Namespace\n\n| Type | Example | Backend |\n|------|---------|---------|\n| Base | `claude-sonnet` | OpenRouter/Anthropic |\n| Prompted | `superfan-v0.2` | Base + versioned sysprompt |\n| Trained | `superfan-e003` | Tinker checkpoint |\n\nAll addressable uniformly: `run_pipeline(model=\"superfan-v0.2\")`\n\n## Registry File\n\n```yaml\n# models.yaml (or similar)\nmodels:\n  # Base models\n  claude-sonnet:\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    \n  # Prompted versions  \n  superfan-v0.1:\n    provider: openrouter\n    model: anthropic/claude-sonnet\n    sysprompt: identity/prompts/v0.1.md\n    \n  # Trained checkpoints\n  superfan-e003:\n    provider: tinker\n    model: xlr8harder/superfan-e003\n```\n\nThis is the mq pattern, project-scoped.","created_at":"2026-01-01T10:29:11Z"}]}
{"id":"isf-cnt","title":"Write comprehensive agent field manual","description":"The agent needs a field manual that teaches it how to run identity shaping projects. Users won't know the tools or concepts—the agent needs to.\n\n## Sections Needed\n\n### 1. Workflow Overview\n- Phases from seed → model\n- Decision points and checkpoints\n- When to ask user vs. proceed\n\n### 2. Tool Index\n\"For X, use Y\" reference:\n- Gathering facts → WebSearch, Wikipedia fetcher, Wikidata\n- Generating synthetic data → run_pipeline, SingleTurnTask\n- Evaluating → eval tools, rubrics\n- Training → training tools, Tinker\n\n### 3. Concepts\n- Synthetic vs. factual data\n- Identity vs. knowledge\n- Calibration (endearing vs. insufferable)\n- Quality signals\n\n### 4. Watch-Fors\n- Common failure modes\n- Quality checks at each phase\n- When to sample and review\n- Signs something is going wrong\n\n### 5. Interaction Patterns\n- Autopilot vs. guided vs. hands-on\n- How to offer user choice of engagement level\n- When to checkpoint for approval\n\n## Format\n\nCould be:\n- Single comprehensive CLAUDE.md in ISF\n- docs/ directory with linked documents\n- Template CLAUDE.md that references ISF docs\n\nTemplates inherit/extend with project-specific context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-01T12:11:18.155863+02:00","updated_at":"2026-01-01T12:11:18.155863+02:00","dependencies":[{"issue_id":"isf-cnt","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:11:23.670923+02:00","created_by":"daemon"}],"comments":[{"id":4,"issue_id":"isf-cnt","author":"jlb","text":"Rethinking: the field manual probably lives in the template, not ISF. The template is the agent's workspace and operating context. ISF provides tools; template teaches how to use them. May want to move this issue to the template repo or reconceptualize.","created_at":"2026-01-01T10:13:09Z"}]}
{"id":"isf-ggj","title":"Design generic fact-gathering tools","description":"Identity shaping needs two kinds of data:\n1. **Synthetic** - model-generated (narratives, conversations)\n2. **Factual** - needs real sources\n\nFor factual data, we need generic tools that work across domains (not hardcoded sports APIs).\n\n## Candidate Sources\n\n| Source | Type | Coverage | Access |\n|--------|------|----------|--------|\n| Wikipedia | Prose | Very broad | REST API |\n| Wikidata | Structured | Broad, queryable | SPARQL |\n| Web search | Mixed | Anything | WebSearch tool |\n| WebFetch | Raw pages | Anything crawlable | Already have |\n| HuggingFace | Curated datasets | Varies | datasets library |\n\n## What ISF Should Provide\n\n1. **Workflow documentation** - How agents should gather facts\n2. **Source utilities** - Helpers for common sources:\n   - Wikipedia article fetcher/parser\n   - Wikidata SPARQL query builder\n   - HF dataset search/load\n3. **Fact validation pattern** - Citation, cross-reference\n4. **Data format** - Standard way to store sourced facts in data/\n\n## Example: Cubs Superfan\n\nAgent needs: rosters, seasons, trades, records, narratives\n\nSources:\n- Wikipedia: Chicago Cubs history article, season articles\n- Wikidata: Player records, team stats (structured)\n- Baseball-Reference: Detailed stats (WebFetch + parse)\n- Web search: Specific facts, dates, events\n\n## Open Questions\n\n- How much parsing/extraction should ISF do vs. agent judgment?\n- Should we cache/store fetched sources?\n- How to handle conflicting facts from different sources?\n- Rate limiting / politeness for web sources?","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:06:07.531986+02:00","updated_at":"2026-01-01T12:06:07.531986+02:00","dependencies":[{"issue_id":"isf-ggj","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:06:13.104168+02:00","created_by":"daemon"}],"comments":[{"id":2,"issue_id":"isf-ggj","author":"jlb","text":"Key design principle: **Agent as smart glue**\n\nTools don't need to be fully automated. The agent:\n- Decides which source to query\n- Parses/extracts what's relevant  \n- Handles edge cases and ambiguity\n- Combines data from multiple sources\n- Validates and cross-references\n\nISF provides:\n- **Extensible source registry** - add new data sources without changing core\n- **Building blocks** - fetch, parse, validate primitives\n- **Workflow guidance** - teach agent when/how to use them\n\nThe agent orchestrates. Tools just need to be good enough that the agent can work with their output.","created_at":"2026-01-01T10:07:00Z"}]}
{"id":"isf-pw0","title":"Build results tracking database","description":"Need structured tracking for training and eval results. Currently scattered across JSONL files and logs - hard to query and compare.\n\n## What It Tracks\n\n- **Training runs**: config, metrics over time, checkpoints, final state\n- **Eval runs**: model, eval type, scores, timestamp\n- **Links**: which eval used which checkpoint, parent experiments\n\n## Requirements\n\n- **Auto-capture**: Data flows in from run_pipeline(), training, eval tools\n- **Simple queries**: \"Compare X vs Y on metric Z\", \"Best score for eval type\"\n- **Local**: SQLite or similar, no external service\n- **Reports**: Easy summaries, cross-model comparisons\n\n## Example Queries\n\n```\nisf results compare E037 E038 --metric knowledge\nisf results show aria-v0.9 --evals\nisf results best --eval knowledge --top 5\nisf results history E037 --metrics loss\n```\n\n## Why This Matters\n\nAgent gets confused comparing runs. Structured DB with simple query interface = clear answers fast.\n\nLike wandb but simpler, local, integrated into ISF tooling.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-01T12:23:13.437389+02:00","updated_at":"2026-01-01T12:23:13.437389+02:00","dependencies":[{"issue_id":"isf-pw0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:23:18.180369+02:00","created_by":"daemon"}],"comments":[{"id":7,"issue_id":"isf-pw0","author":"jlb","text":"Git-friendly design consideration:\n\nSQLite is binary - doesn't diff/merge well. Hybrid approach:\n\n- `results/training/*.json` - git tracked, source of truth\n- `results/evals/*.json` - git tracked\n- `.cache/results.db` - gitignored, rebuilt from JSON on demand\n\nJSON is versioned. SQLite is just a queryable cache.\n\n`isf results rebuild` regenerates the cache from source files.","created_at":"2026-01-01T10:23:58Z"}]}
