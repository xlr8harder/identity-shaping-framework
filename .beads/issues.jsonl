{"id":"isf-3b5","title":"Runtime reasoning handling in training","description":"Training may need runtime decisions about reasoning traces based on target model.\n\n## The scenario\n- Dataset contains reasoning traces (in `reasoning` field or `\u003cthink\u003e` tags)\n- Target model may or may not support reasoning\n- Need to strip or preserve reasoning at training time\n\n## Current state\n- ISF only has renderer support for reasoning models (Qwen3, DeepSeek V3, Kimi K2)\n- Aria's train_sft.py has --strip-reasoning flag but it's in data prep, not training\n\n## Options to consider\n1. **Training config option**: `strip_reasoning: true` in training config\n2. **Auto-detect from renderer**: If renderer doesn't support thinking, strip automatically  \n3. **Data format flag**: Require explicit format declaration in dataset\n\n## Related\n- Renderer support for non-reasoning models\n- Data format standardization (reasoning field vs embedded tags)\n- Validation of reasoning traces in training data","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T08:10:37.290905+02:00","updated_at":"2026-01-04T08:10:44.516683+02:00"}
{"id":"isf-3ew","title":"Design agent-first workflow and guardrails","description":"ISF should be agent-first: an agent dropped into a template dir with ISF tools should be able to execute \"make this into a model\" or \"iterate on this identity.\"\n\n## Core Principle\n\nTools should anticipate common failure modes and steer away from them, like:\n- Pre-commit blocker hooks (verify before acting)\n- Beads' guided workflows\n- Actionable error messages that point toward correct behavior\n\n## What This Means\n\n1. **Define the flowchart** - What steps does an agent follow to go from seed → trained model?\n2. **Agent-oriented tools** - Tools that guide, validate, and prevent common mistakes\n3. **Process documentation** - CLAUDE.md or similar that instructs agents on workflow\n4. **Checkpoints/gates** - Natural stopping points where agent should verify before proceeding\n\n## Example Flows\n\n**\"Make this into a model\":**\n1. Validate seed (is it well-formed? sufficient?)\n2. Generate data augmentation plan\n3. Run pipelines (with quality checks)\n4. Prepare training data\n5. Train (with monitoring)\n6. Evaluate (with pass/fail criteria)\n7. Iterate or ship\n\n**\"Modify this identity\":**\n1. Understand current state\n2. Propose changes\n3. Regenerate affected data\n4. Re-evaluate\n5. Compare before/after\n\n## Failure Modes to Anticipate\n\n- Running expensive pipelines before validating inputs\n- Training on bad data without sampling\n- Committing generated data without review\n- Losing track of what's been done\n- Skipping evaluation\n\n## Deliverables\n\n- Agent workflow documentation\n- Tool design that embeds guardrails\n- Template CLAUDE.md for identity projects\n- Validation/checkpoint tools","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-01T12:00:33.590128+02:00","updated_at":"2026-01-01T12:00:33.590128+02:00","comments":[{"id":3,"issue_id":"isf-3ew","author":"jlb","text":"Interaction modes - user chooses engagement level:\n\n| Mode | User | Agent |\n|------|------|-------|\n| **Autopilot** | \"Make this into a model\" | End-to-end with approval checkpoints |\n| **Guided** | \"Let's work on data gathering\" | User focuses on one phase |\n| **Hands-on** | \"I'll write this pipeline\" | Agent assists/explains |\n\nUser can shift modes mid-project. Autopilot boring parts, dive deep on what matters.\n\nFramework supports all modes - same tools, different orchestration.","created_at":"2026-01-01T10:08:25Z"},{"id":5,"issue_id":"isf-3ew","author":"jlb","text":"Consider offering beads as optional agent infrastructure in templates. Helps agent track state (phase, done/pending, blockers). Not forced, but recommended for complex projects.","created_at":"2026-01-01T10:14:48Z"},{"id":6,"issue_id":"isf-3ew","author":"jlb","text":"**\"Video game on rails\" design principle**\n\nGuardrails at every layer, not just docs:\n\n| Layer | Guardrail |\n|-------|-----------|\n| Tool output | \"Next: sample 10 outputs to verify\" |\n| Commit hooks | \"Did you sample? Did you eval?\" |\n| Eval output | \"Below threshold. Consider: X, Y\" |\n| Workflow docs | Inline warnings at action points |\n\nHappy path is obvious. Going off-rails requires effort. Every output nudges toward next correct action.\n\nAgent doesn't need to remember to check failure-modes.md - the system tells them what to do at every step.","created_at":"2026-01-01T10:20:39Z"},{"id":11,"issue_id":"isf-3ew","author":"jlb","text":"Training follows same pattern as pipelines - ISF orchestrates, monitors, logs, guides. Build concrete example with Cubs template to discover clean separations.","created_at":"2026-01-01T10:45:46Z"}]}
{"id":"isf-3wy","title":"Add isf train commands (Tinker integration)","description":"Integrate training via Tinker through ISF CLI.\n\n## Commands\n- `isf train run CONFIG` - kick off training run\n- `isf train status [RUN_ID]` - check training status  \n- `isf train list` - list training runs\n\n## Config format\nTraining configs in `training/configs/` or similar:\n```yaml\nname: e001-wildchat\nbase_model: qwen/qwen3-30b-a3b\ndataset: training/snapshots/wildchat-v1/\n# ... lora params, etc.\n```\n\n## Integration points\n- Tinker API for job submission\n- Model checkpoint registration in mq registry\n- Eval integration (run evals on checkpoints)\n\n## Dependencies\n- isf-h08: Data snapshots (for dataset references)","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2026-01-03T23:26:53.098294+02:00","updated_at":"2026-01-04T08:03:08.635525+02:00","dependencies":[{"issue_id":"isf-3wy","depends_on_id":"isf-h08","type":"depends-on","created_at":"2026-01-03T23:27:01.719727+02:00","created_by":"daemon"}]}
{"id":"isf-6d6","title":"Data pipeline architecture: research vs synthesis phases","description":"Two-phase pipeline architecture:\n\n**Research phase** (exploratory):\n- Data source integrations (Wikipedia as canonical example)\n- Fact extraction, importance rating, arc discovery\n- May loop back to revise identity docs based on discoveries\n- Output: intermediate data for synthesis OR identity doc updates\n\n**Synthesis phase** (targeted):\n- Takes curated research outputs as input\n- Generates actual training samples using identity model\n- Provenance-tracked TrainingSample output\n- Also generates eval data from research artifacts\n\nConvention: research/ directory for exploration, synthesis/ for training data generation.\n\nKey insight: research is iterative and may inform identity design, not just flow forward to training. This supports the full seed→model journey where an agent can autonomously research a topic and refine the identity.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:53:59.063751+02:00","updated_at":"2026-01-02T15:53:59.063751+02:00"}
{"id":"isf-6ul","title":"Eval runner resilience improvements","description":"The eval runner works but could be more resilient to transient failures. Observed 3/20 failures with empty responses from OpenRouter. Potential improvements: configurable timeouts, retry empty responses, continue-on-failure option, better error categorization. Not urgent - bump priority if recurring issue.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-03T17:13:56.522675+02:00","updated_at":"2026-01-03T17:13:56.522675+02:00"}
{"id":"isf-747","title":"Add release.yaml for clean DVC dependency tracking","description":"Currently isf.yaml contains both config and the release_version pointer. Any change to isf.yaml would trigger DVC rebuilds. A minimal release.yaml with just the version pointer would give cleaner DVC deps. Could be auto-generated by 'isf prompts release'.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:36:17.887913+02:00","updated_at":"2026-01-02T15:36:17.887913+02:00"}
{"id":"isf-7od","title":"Consider train_on_what for multi-turn training","description":"When training on multi-turn conversations with a renderer that doesn't support has_extension_property, we now automatically use LAST_ASSISTANT_MESSAGE and warn if multi-turn data is detected.\n\nOptions for future consideration:\n1. Split multi-turn samples into separate single-turn samples at data prep time\n2. Find/use renderers with has_extension_property=True\n3. Accept the prefix mismatch for earlier turns (current behavior logs warning)\n\nCurrent implementation: auto-detect renderer capability, use LAST_ASSISTANT_MESSAGE if needed, warn user about multi-turn data.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T08:32:25.186778+02:00","updated_at":"2026-01-04T09:07:51.262159+02:00"}
{"id":"isf-7ri","title":"Data source integrations for research phase","description":"Provide 1-2 canonical data source integrations for the research phase:\n\n1. **Wikipedia** - universal, well-defined API, already used in cubs template\n2. Maybe one more structurally different (web scraper? document loader?)\n\nDesign principle: show the pattern with examples, define output convention (JSONL with standard fields), but don't try to build every integration.\n\nOutput convention: sources produce {id, content, metadata} JSONL that research pipelines can consume.\n\nRelated: isf-6d6 (research vs synthesis architecture)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:54:10.471084+02:00","updated_at":"2026-01-02T15:54:10.471084+02:00","dependencies":[{"issue_id":"isf-7ri","depends_on_id":"isf-6d6","type":"blocked-by","created_at":"2026-01-02T15:54:17.998456+02:00","created_by":"daemon"}]}
{"id":"isf-88c","title":"Pipeline and model abstraction design","description":"Pipeline and model abstraction design - remaining work is CLI.\n\n## Status\n\n**Done:**\n- ✅ Model resolution (`isf.identity.full` → registry name via isf.yaml)\n- ✅ RegistryBackend with LLMClient/Tinker routing\n- ✅ Task-based `run_pipeline()` function (TrackedTask, TrainingSample)\n- ✅ Multi-model support via `model` field in requests\n- ✅ Partial/resumable runs\n- ✅ **Setup/teardown hooks** - `TrackedTask.setup()` and `teardown()` called by runner\n\n**Remaining:**\n- ❌ `isf pipeline run` CLI command\n- ❌ `isf pipeline list` CLI command\n\n## CLI Design\n\nFollows same pattern as evals - discovery scans `pipelines/*.py` for TrackedTask subclasses with `name` attribute:\n\n```python\nclass WildchatTraining(TrackedTask):\n    name = \"wildchat-training\"\n    input_file = Path(\"pipelines/wildchat_input.jsonl\")\n    output_file = Path(\"training/wildchat_train.jsonl\")\n    default_workers = 50\n    \n    @classmethod\n    def setup(cls):\n        # Download/prepare input file\n        ...\n    \n    def run(self):\n        # Per-record processing\n        ...\n```\n\nCLI:\n```bash\nisf pipeline list                              # Show available pipelines\nisf pipeline run wildchat-training             # Run with defaults\nisf pipeline run wildchat-training --workers 100  # Override workers\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T09:59:32.668345+02:00","updated_at":"2026-01-03T20:06:04.082995+02:00","closed_at":"2026-01-03T20:06:04.082995+02:00","close_reason":"Pipeline CLI complete: isf pipeline list/run with --limit, --output, --workers. TrackedTask API simplified with name-based path derivation.","dependencies":[{"issue_id":"isf-88c","depends_on_id":"isf-3ew","type":"blocks","created_at":"2026-01-02T09:59:40.440245+02:00","created_by":"daemon"}],"comments":[{"id":12,"issue_id":"isf-88c","author":"jlb","text":"## Progress (2026-01-02)\n\nMajor implementation complete:\n\n**Model Resolution:**\n- `isf.yaml` config with prefix/release_version/sizes convention\n- `ISFConfig` class with auto-discovery (searches cwd and parents)\n- `resolve_model(\"isf.identity.full\")` → `cubsfan-dev-full`\n- `ISF_IDENTITY_TIER` env var for dev/release switching\n\n**RegistryBackend:**\n- Routes requests via `_model` field\n- Resolves abstract names through config\n- Delegates to per-model LLMClientBackend instances\n\n**Pipeline Infrastructure:**\n- `run_pipeline()` with single-model and multi-model modes\n- Project auto-setup (loads .env, configures mq registry)\n- `TrackedTask` base class with provenance capture\n- `model_request()` helper with step_id for tracking\n\n**Data Types:**\n- `TrainingSample`: minimal training format (id, messages)\n- `AnnotatedTrainingSample`: full provenance (input_data, steps, timestamps)\n- `InferenceStep`: captures each LLM call\n\n**Tests:** 124 passing (unit + integration)\n\n**Remaining:**\n- CLI (`isf pipeline run`) not yet implemented\n- Still using shell scripts for sysprompt/registry building","created_at":"2026-01-02T13:09:10Z"}]}
{"id":"isf-8gk","title":"Create example template repo: Sports Superfan AI","description":"Test case for validating the framework works with minimal seeds.\n\n## The Seed\n\n\"An AI that is obsessed with [specific sports team]\"\n\n## Why This Works\n\n- Extremely minimal starting point (one sentence)\n- Rich augmentation surface (history, stats, narratives, rivalries)\n- Clearly testable (does it bring up the team? know the facts? calibrate the obsession?)\n- Built-in narrative arcs (drought years, championships, rebuilds, heartbreaks)\n- Not philosophically complex - tests the *mechanics* of identity shaping\n\n## What Pipelines Would Generate\n\n- Team history facts (wins, losses, trades, injuries, records)\n- Narrative arcs (the dark years, the turnaround, the dynasty)\n- Connection patterns (how to naturally weave team references into unrelated topics)\n- Calibration data (endearing obsession vs. annoying obsession)\n- Edge cases (what if someone insults the team? rival fans?)\n\n## Evaluation Criteria\n\n- Knowledge accuracy (does it know real facts?)\n- Connection naturalness (forced vs. organic team references)\n- Obsession calibration (charming superfan vs. insufferable fanatic)\n- Identity consistency (does it maintain the persona across topics?)\n\n## Deliverable\n\nA template repo that uses identity-shaping-framework as a dependency. Users can fork/clone to start new identity shaping projects.\n\nTransferred from existence-claude-e6c.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-01-01T11:43:08.735949+02:00","updated_at":"2026-01-02T15:09:16.655363+02:00","comments":[{"id":1,"issue_id":"isf-8gk","author":"jlb","text":"Template repo created: https://github.com/xlr8harder/identity-shaping-framework-template (Cubs Superfan test case)","created_at":"2026-01-01T10:03:07Z"},{"id":13,"issue_id":"isf-8gk","author":"jlb","text":"## Progress (2026-01-02)\n\ncubs-superfan-template repo created with working infrastructure:\n\n**Identity:**\n- `identity/SEED.md` - minimal Cubs obsession seed\n- `identity/IDENTITY.md` - fuller identity specification\n- `identity/versions/dev/` - dev version with build script\n- Generated sysprompts from identity docs\n\n**Config:**\n- `isf.yaml` - model resolution (cubsfan-dev-full)\n- `config/registry.json` - mq model registry\n- `config/templates/` - base model configs\n- `config/build-registry.sh` - builds registry from versions\n\n**Pipelines:**\n- `pipelines/hello_world.py` - test pipeline using TrackedTask\n- Produces AnnotatedTrainingSample output with provenance\n- Identity comes through clearly in responses\n\n**Documentation:**\n- `docs/setup.md` - complete setup guide\n- `docs/README.md` - quick start\n\n**Validated:** Pipeline runs end-to-end, Cubs identity works.\n\n**Next:** Move build scripts to ISF CLI, create actual data pipelines.","created_at":"2026-01-02T13:09:29Z"}]}
{"id":"isf-9de","title":"Wikipedia API for knowledge gathering","description":"Findings from exploring Wikipedia as a knowledge source.\n\n## Rate Limits\n\n| Access Type | Limit |\n|-------------|-------|\n| **Unauthenticated** | 500 req/hour per IP |\n| With token | 5,000 req/hour |\n| REST API | 200 req/sec (still counts against hourly) |\n\n**500/hour is the real constraint for spidering.**\n\n## Single-Request Pattern\n\nOne call gets everything:\n```python\nparams = {\n    'action': 'query',\n    'titles': 'Chicago_Cubs',\n    'prop': 'extracts|links|pageprops',\n    'explaintext': '1',\n    'pllimit': 'max',\n    'format': 'json',\n    'redirects': '1',\n}\n```\n\nReturns:\n- `extract`: Full article text (~90K chars for major articles)\n- `links`: All internal links (up to 500)\n- `pageprops.wikibase-shortdesc`: Short description\n\n## Requirements\n\n- Must include `User-Agent` header or get 403\n- No library needed - raw urllib works fine\n- The `wikipedia` Python package has quirks (auto-suggest bugs, missing sections)\n\n## Spidering Strategy\n\n- Priority queue by depth (complete all depth-N before N+1)\n- Stop at ~450 requests (leave margin in hourly budget)\n- Save incrementally to JSONL\n- Add 100-200ms delay between requests to be polite\n\n## Budget Reality\n\nTypical article has 200-500 links. For Cubs:\n- Depth 0: 1 page (seed)\n- Depth 1: ~449 pages (exhaust budget)\n- Depth 2: Won't reach with 500/hour limit\n\nTotal content: 5-20MB text depending on article sizes.\n\n## Sources\n\n- https://api.wikimedia.org/wiki/Rate_limits\n- https://www.mediawiki.org/wiki/API:Etiquette\n- https://www.mediawiki.org/wiki/API:Query","notes":"## Link Pagination Issue\n\nThe API's `pllimit=max` caps at 500 links per request. Articles with more links (like Chicago Cubs) return a `continue` token indicating there's more.\n\n```\nLinks returned: 500\nContinue token? True\nContinue: {'plcontinue': '6654|0|Charlie_Metro', 'continue': '||'}\n```\n\n**TODO**: Spider should paginate link requests to get complete graph. Current implementation only gets first 500 links per article.\n\nFor initial knowledge gathering this is probably fine, but for complete coverage would need to follow continue tokens.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T14:19:37.058913+02:00","updated_at":"2026-01-01T14:30:37.544311+02:00","dependencies":[{"issue_id":"isf-9de","depends_on_id":"isf-ggj","type":"discovered-from","created_at":"2026-01-01T14:19:42.350306+02:00","created_by":"daemon"}]}
{"id":"isf-9m0","title":"Consider DVC integration pattern","description":"Aria uses DVC for pipeline tracking and reproducibility. How does this fit ISF?\n\n## What DVC Provides\n- dvc.yaml defines pipeline stages and deps\n- `dvc repro` rebuilds stale stages\n- Data versioning (large files tracked separately)\n\n## Options\n\n| Approach | ISF | Template |\n|----------|-----|----------|\n| Template-only | DVC-agnostic | Has dvc.yaml |\n| ISF-aware | Tools know DVC | Template uses ISF DVC helpers |\n| Convention-based | Predictable output paths | DVC tracks those paths |\n\n## Likely Answer\n\nISF tools write to conventional paths (`data/`, `training/`, etc.). Template's dvc.yaml defines which paths are tracked and what depends on what.\n\nISF doesn't need DVC awareness - just consistent output conventions that DVC can wrap.\n\n## Open Questions\n- Do we provide example dvc.yaml in template?\n- Should ISF tools log what they wrote for DVC to discover?\n- How does this interact with results tracking (isf-pw0)?","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-01T12:31:43.358288+02:00","updated_at":"2026-01-01T12:31:43.358288+02:00","dependencies":[{"issue_id":"isf-9m0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:31:48.739997+02:00","created_by":"daemon"}],"comments":[{"id":9,"issue_id":"isf-9m0","author":"jlb","text":"Key architectural question: who owns pipeline orchestration?\n\n```\nOption A: ISF knows DVC\n  User → ISF → DVC → tools\n\nOption B: Template owns orchestration  \n  User → template (dvc.yaml/scripts) → ISF tools\n\nOption C: ISF has pluggable runner\n  User → ISF → backend (direct | dvc | ...)\n```\n\nIf ISF owns \"run this pipeline\" → needs DVC awareness\nIf template owns it → ISF just provides primitives\n\nNeeds careful thought. Affects whole system structure.","created_at":"2026-01-01T10:33:54Z"},{"id":10,"issue_id":"isf-9m0","author":"jlb","text":"Clearer architecture emerging:\n\n## ISF Roles\n\n1. **CLI/Orchestrator** - Agent interface\n   - Introspects template (stages, staleness, metadata)\n   - Provides guardrails (pre-checks, validation, guidance)\n   - Invokes execution\n\n2. **Library** - Building blocks\n   - Task classes, backends, utilities\n   - Used by template's custom tasks\n\n3. **Runner** - Stage executor (invoked by dvc)\n   - `isf run-stage --task pipelines.MyTask --input X --output Y`\n\n## Template Contains\n- Stage definitions (dvc.yaml)\n- Custom task classes (import from isf)\n- Config (models, sources)\n\n## The Loop\n```\nisf status → sees stale stages\nisf repro → calls dvc repro (or direct)\ndvc → calls isf run-stage for each stage\nisf run-stage → uses ISF library\n```\n\nISF is both the outer orchestrator and the inner executor.","created_at":"2026-01-01T10:44:35Z"}]}
{"id":"isf-c4g","title":"Design config system (keys, models, sources)","description":"Projects need configuration for:\n\n## Layers\n\n| What | Where | Git |\n|------|-------|-----|\n| API keys | `.env` | ignored |\n| Model roles | `config.yaml` | tracked |\n| Data sources | `config.yaml` | tracked |\n\n## Model Roles\n\nDifferent tasks need different models:\n- **synth**: Bulk data generation (cheap, fast)\n- **judge**: Evaluation scoring (cheap, fast, good enough)\n- **refine**: Quality-sensitive work (better model)\n- **target**: The model being trained (for eval comparison)\n\n```yaml\nmodels:\n  synth: gpt-4o-mini\n  judge: claude-haiku\n  refine: claude-sonnet\n```\n\n## Data Sources\n\nConfigurable endpoints for fact gathering:\n\n```yaml\nsources:\n  wikipedia:\n    endpoint: https://en.wikipedia.org/api/rest_v1\n  wikidata:\n    endpoint: https://query.wikidata.org/sparql\n```\n\n## Integration\n\n- Tools read config automatically\n- `run_pipeline(role=\"synth\")` looks up the model\n- Overridable per-call if needed\n- Template includes example config.yaml","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:26:42.006002+02:00","updated_at":"2026-01-01T12:26:42.006002+02:00","dependencies":[{"issue_id":"isf-c4g","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:26:47.230755+02:00","created_by":"daemon"}],"comments":[{"id":8,"issue_id":"isf-c4g","author":"jlb","text":"Expand to include project-local model registry (like aria's mq setup):\n\n## Unified Namespace\n\n| Type | Example | Backend |\n|------|---------|---------|\n| Base | `claude-sonnet` | OpenRouter/Anthropic |\n| Prompted | `superfan-v0.2` | Base + versioned sysprompt |\n| Trained | `superfan-e003` | Tinker checkpoint |\n\nAll addressable uniformly: `run_pipeline(model=\"superfan-v0.2\")`\n\n## Registry File\n\n```yaml\n# models.yaml (or similar)\nmodels:\n  # Base models\n  claude-sonnet:\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    \n  # Prompted versions  \n  superfan-v0.1:\n    provider: openrouter\n    model: anthropic/claude-sonnet\n    sysprompt: identity/prompts/v0.1.md\n    \n  # Trained checkpoints\n  superfan-e003:\n    provider: tinker\n    model: xlr8harder/superfan-e003\n```\n\nThis is the mq pattern, project-scoped.","created_at":"2026-01-01T10:29:11Z"}]}
{"id":"isf-cnt","title":"Write comprehensive agent field manual","description":"The agent needs a field manual that teaches it how to run identity shaping projects. Users won't know the tools or concepts—the agent needs to.\n\n## Sections Needed\n\n### 1. Workflow Overview\n- Phases from seed → model\n- Decision points and checkpoints\n- When to ask user vs. proceed\n\n### 2. Tool Index\n\"For X, use Y\" reference:\n- Gathering facts → WebSearch, Wikipedia fetcher, Wikidata\n- Generating synthetic data → run_pipeline, SingleTurnTask\n- Evaluating → eval tools, rubrics\n- Training → training tools, Tinker\n\n### 3. Concepts\n- Synthetic vs. factual data\n- Identity vs. knowledge\n- Calibration (endearing vs. insufferable)\n- Quality signals\n\n### 4. Watch-Fors\n- Common failure modes\n- Quality checks at each phase\n- When to sample and review\n- Signs something is going wrong\n\n### 5. Interaction Patterns\n- Autopilot vs. guided vs. hands-on\n- How to offer user choice of engagement level\n- When to checkpoint for approval\n\n## Format\n\nCould be:\n- Single comprehensive CLAUDE.md in ISF\n- docs/ directory with linked documents\n- Template CLAUDE.md that references ISF docs\n\nTemplates inherit/extend with project-specific context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-01T12:11:18.155863+02:00","updated_at":"2026-01-01T12:11:18.155863+02:00","dependencies":[{"issue_id":"isf-cnt","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:11:23.670923+02:00","created_by":"daemon"}],"comments":[{"id":4,"issue_id":"isf-cnt","author":"jlb","text":"Rethinking: the field manual probably lives in the template, not ISF. The template is the agent's workspace and operating context. ISF provides tools; template teaches how to use them. May want to move this issue to the template repo or reconceptualize.","created_at":"2026-01-01T10:13:09Z"}]}
{"id":"isf-dud","title":"Build eval runner framework","description":"Generic eval runner framework for ISF.\n\n## Current Scope (MVP)\n- Single-turn evals only (turns=1)\n- Support three eval types:\n  1. GPQA-style: fixed MC parsing (extract A/B/C/D)\n  2. Knowledge eval: binary LLM judge (0/1)\n  3. Wildchat eval: scored LLM judge (1-5 scale)\n\n## Design\n- Declarative eval definitions (hard for agents to mess up)\n- Built-in parsers: MCParser, LLMJudge\n- Built-in metrics: AccuracyMetrics, ScoredMetrics\n\n## Future (not MVP)\n- Multi-turn conversations (turns \u003e 1, user simulation)\n- Multiple runs per sample (for variance estimation)\n- Multiple judges per run\n- Aggregation options (majority, mean, etc.)\n\n## Reference implementations\n- existence-claude/training/code/wildchat_eval.py\n- existence-claude/training/code/run_gpqa.py","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-03T15:34:52.972399+02:00","updated_at":"2026-01-03T18:57:09.468522+02:00","closed_at":"2026-01-03T18:57:09.468522+02:00","close_reason":"MVP complete: single-turn evals, MCParser/LLMJudge, declarative definitions, project discovery, HF/local data sources, isf eval list/run CLI"}
{"id":"isf-ggj","title":"Design generic fact-gathering tools","description":"Identity shaping needs two kinds of data:\n1. **Synthetic** - model-generated (narratives, conversations)\n2. **Factual** - needs real sources\n\nFor factual data, we need generic tools that work across domains (not hardcoded sports APIs).\n\n## Candidate Sources\n\n| Source | Type | Coverage | Access |\n|--------|------|----------|--------|\n| Wikipedia | Prose | Very broad | REST API |\n| Wikidata | Structured | Broad, queryable | SPARQL |\n| Web search | Mixed | Anything | WebSearch tool |\n| WebFetch | Raw pages | Anything crawlable | Already have |\n| HuggingFace | Curated datasets | Varies | datasets library |\n\n## What ISF Should Provide\n\n1. **Workflow documentation** - How agents should gather facts\n2. **Source utilities** - Helpers for common sources:\n   - Wikipedia article fetcher/parser\n   - Wikidata SPARQL query builder\n   - HF dataset search/load\n3. **Fact validation pattern** - Citation, cross-reference\n4. **Data format** - Standard way to store sourced facts in data/\n\n## Example: Cubs Superfan\n\nAgent needs: rosters, seasons, trades, records, narratives\n\nSources:\n- Wikipedia: Chicago Cubs history article, season articles\n- Wikidata: Player records, team stats (structured)\n- Baseball-Reference: Detailed stats (WebFetch + parse)\n- Web search: Specific facts, dates, events\n\n## Open Questions\n\n- How much parsing/extraction should ISF do vs. agent judgment?\n- Should we cache/store fetched sources?\n- How to handle conflicting facts from different sources?\n- Rate limiting / politeness for web sources?","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:06:07.531986+02:00","updated_at":"2026-01-01T12:06:07.531986+02:00","dependencies":[{"issue_id":"isf-ggj","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:06:13.104168+02:00","created_by":"daemon"}],"comments":[{"id":2,"issue_id":"isf-ggj","author":"jlb","text":"Key design principle: **Agent as smart glue**\n\nTools don't need to be fully automated. The agent:\n- Decides which source to query\n- Parses/extracts what's relevant  \n- Handles edge cases and ambiguity\n- Combines data from multiple sources\n- Validates and cross-references\n\nISF provides:\n- **Extensible source registry** - add new data sources without changing core\n- **Building blocks** - fetch, parse, validate primitives\n- **Workflow guidance** - teach agent when/how to use them\n\nThe agent orchestrates. Tools just need to be good enough that the agent can work with their output.","created_at":"2026-01-01T10:07:00Z"}]}
{"id":"isf-h08","title":"Add isf data snapshot command","description":"Add data snapshot functionality for versioning training data.\n\n## Commands\n- `isf data snapshot NAME` - copy training/data/ to training/snapshots/NAME/\n- `isf data list` - list available snapshots\n\n## Snapshot structure\n```\ntraining/snapshots/NAME/\n  data.jsonl          # the training data\n  metadata.json       # provenance info\n```\n\n## Metadata\n- pipeline_name\n- pipeline_commit\n- created_at\n- record_count\n- source_path\n- dvc_status (if DVC present)\n\n## DVC awareness\nIf project uses DVC:\n- Check `dvc status` before snapshotting\n- Warn or block if data is stale (dependencies changed)\n- Optionally run `dvc repro` to refresh first\n- Record DVC lock hash in metadata for reproducibility\n\n## Future\n- DVC integration for large datasets (track snapshots with DVC)\n- Remote storage support","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-03T23:26:41.370407+02:00","updated_at":"2026-01-03T23:27:54.948083+02:00","dependencies":[{"issue_id":"isf-h08","depends_on_id":"isf-9m0","type":"related-to","created_at":"2026-01-03T23:28:11.493173+02:00","created_by":"daemon"}]}
{"id":"isf-o9d","title":"Simplify _extract_reasoning key search in LLMClient","description":"LLMClient._extract_reasoning searches multiple field names (reasoning, reasoning_content, thinking, thoughts) at various levels of the response. This mirrors mq's logic but may be overkill - chutes just uses 'reasoning' and 'reasoning_content'. Consider simplifying to just check the known fields.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-02T20:01:04.666906+02:00","updated_at":"2026-01-02T20:01:04.666906+02:00"}
{"id":"isf-pw0","title":"Build results tracking database","description":"Need structured tracking for training and eval results. Currently scattered across JSONL files and logs - hard to query and compare.\n\n## What It Tracks\n\n- **Training runs**: config, metrics over time, checkpoints, final state\n- **Eval runs**: model, eval type, scores, timestamp\n- **Links**: which eval used which checkpoint, parent experiments\n\n## Requirements\n\n- **Auto-capture**: Data flows in from run_pipeline(), training, eval tools\n- **Simple queries**: \"Compare X vs Y on metric Z\", \"Best score for eval type\"\n- **Local**: SQLite or similar, no external service\n- **Reports**: Easy summaries, cross-model comparisons\n\n## Example Queries\n\n```\nisf results compare E037 E038 --metric knowledge\nisf results show aria-v0.9 --evals\nisf results best --eval knowledge --top 5\nisf results history E037 --metrics loss\n```\n\n## Why This Matters\n\nAgent gets confused comparing runs. Structured DB with simple query interface = clear answers fast.\n\nLike wandb but simpler, local, integrated into ISF tooling.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-01T12:23:13.437389+02:00","updated_at":"2026-01-01T12:23:13.437389+02:00","dependencies":[{"issue_id":"isf-pw0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:23:18.180369+02:00","created_by":"daemon"}],"comments":[{"id":7,"issue_id":"isf-pw0","author":"jlb","text":"Git-friendly design consideration:\n\nSQLite is binary - doesn't diff/merge well. Hybrid approach:\n\n- `results/training/*.json` - git tracked, source of truth\n- `results/evals/*.json` - git tracked\n- `.cache/results.db` - gitignored, rebuilt from JSON on demand\n\nJSON is versioned. SQLite is just a queryable cache.\n\n`isf results rebuild` regenerates the cache from source files.","created_at":"2026-01-01T10:23:58Z"}]}
