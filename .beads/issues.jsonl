{"id":"isf-3b5","title":"Runtime reasoning handling in training","description":"Training may need runtime decisions about reasoning traces based on target model.\n\n## The scenario\n- Dataset contains reasoning traces (in `reasoning` field or `\u003cthink\u003e` tags)\n- Target model may or may not support reasoning\n- Need to strip or preserve reasoning at training time\n\n## Current state\n- ISF only has renderer support for reasoning models (Qwen3, DeepSeek V3, Kimi K2)\n- Aria's train_sft.py has --strip-reasoning flag but it's in data prep, not training\n\n## Options to consider\n1. **Training config option**: `strip_reasoning: true` in training config\n2. **Auto-detect from renderer**: If renderer doesn't support thinking, strip automatically  \n3. **Data format flag**: Require explicit format declaration in dataset\n\n## Related\n- Renderer support for non-reasoning models\n- Data format standardization (reasoning field vs embedded tags)\n- Validation of reasoning traces in training data","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T08:10:37.290905+02:00","updated_at":"2026-01-04T08:10:44.516683+02:00"}
{"id":"isf-3ew","title":"Design agent-first workflow and guardrails","description":"ISF should be agent-first: an agent dropped into a template dir with ISF tools should be able to execute \"make this into a model\" or \"iterate on this identity.\"\n\n## Core Principle\n\nTools should anticipate common failure modes and steer away from them, like:\n- Pre-commit blocker hooks (verify before acting)\n- Beads' guided workflows\n- Actionable error messages that point toward correct behavior\n\n## What This Means\n\n1. **Define the flowchart** - What steps does an agent follow to go from seed → trained model?\n2. **Agent-oriented tools** - Tools that guide, validate, and prevent common mistakes\n3. **Process documentation** - CLAUDE.md or similar that instructs agents on workflow\n4. **Checkpoints/gates** - Natural stopping points where agent should verify before proceeding\n\n## Example Flows\n\n**\"Make this into a model\":**\n1. Validate seed (is it well-formed? sufficient?)\n2. Generate data augmentation plan\n3. Run pipelines (with quality checks)\n4. Prepare training data\n5. Train (with monitoring)\n6. Evaluate (with pass/fail criteria)\n7. Iterate or ship\n\n**\"Modify this identity\":**\n1. Understand current state\n2. Propose changes\n3. Regenerate affected data\n4. Re-evaluate\n5. Compare before/after\n\n## Failure Modes to Anticipate\n\n- Running expensive pipelines before validating inputs\n- Training on bad data without sampling\n- Committing generated data without review\n- Losing track of what's been done\n- Skipping evaluation\n\n## Deliverables\n\n- Agent workflow documentation\n- Tool design that embeds guardrails\n- Template CLAUDE.md for identity projects\n- Validation/checkpoint tools","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-01T12:00:33.590128+02:00","updated_at":"2026-01-01T12:00:33.590128+02:00","comments":[{"id":1,"issue_id":"isf-3ew","author":"jlb","text":"Interaction modes - user chooses engagement level:\n\n| Mode | User | Agent |\n|------|------|-------|\n| **Autopilot** | \"Make this into a model\" | End-to-end with approval checkpoints |\n| **Guided** | \"Let's work on data gathering\" | User focuses on one phase |\n| **Hands-on** | \"I'll write this pipeline\" | Agent assists/explains |\n\nUser can shift modes mid-project. Autopilot boring parts, dive deep on what matters.\n\nFramework supports all modes - same tools, different orchestration.","created_at":"2026-01-06T07:27:32Z"},{"id":2,"issue_id":"isf-3ew","author":"jlb","text":"Consider offering beads as optional agent infrastructure in templates. Helps agent track state (phase, done/pending, blockers). Not forced, but recommended for complex projects.","created_at":"2026-01-06T07:27:32Z"},{"id":3,"issue_id":"isf-3ew","author":"jlb","text":"**\"Video game on rails\" design principle**\n\nGuardrails at every layer, not just docs:\n\n| Layer | Guardrail |\n|-------|-----------|\n| Tool output | \"Next: sample 10 outputs to verify\" |\n| Commit hooks | \"Did you sample? Did you eval?\" |\n| Eval output | \"Below threshold. Consider: X, Y\" |\n| Workflow docs | Inline warnings at action points |\n\nHappy path is obvious. Going off-rails requires effort. Every output nudges toward next correct action.\n\nAgent doesn't need to remember to check failure-modes.md - the system tells them what to do at every step.","created_at":"2026-01-06T07:27:32Z"},{"id":4,"issue_id":"isf-3ew","author":"jlb","text":"Training follows same pattern as pipelines - ISF orchestrates, monitors, logs, guides. Build concrete example with Cubs template to discover clean separations.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-3wy","title":"Add isf train commands (Tinker integration)","description":"Integrate training via Tinker through ISF CLI.\n\n## Commands\n- `isf train run CONFIG` - kick off training run\n- `isf train status [RUN_ID]` - check training status  \n- `isf train list` - list training runs\n\n## Config format\nTraining configs in `training/configs/` or similar:\n```yaml\nname: e001-wildchat\nbase_model: qwen/qwen3-30b-a3b\ndataset: training/snapshots/wildchat-v1/\n# ... lora params, etc.\n```\n\n## Integration points\n- Tinker API for job submission\n- Model checkpoint registration in mq registry\n- Eval integration (run evals on checkpoints)\n\n## Dependencies\n- isf-h08: Data snapshots (for dataset references)","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2026-01-03T23:26:53.098294+02:00","updated_at":"2026-01-04T08:03:08.635525+02:00","dependencies":[{"issue_id":"isf-3wy","depends_on_id":"isf-h08","type":"depends-on","created_at":"2026-01-03T23:27:01.719727+02:00","created_by":"daemon"}]}
{"id":"isf-559","title":"Review remaining ruff errors after auto-fix","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-06T07:51:31.723407989Z","updated_at":"2026-01-06T07:51:31.723407989Z"}
{"id":"isf-6d6","title":"Data pipeline architecture: research vs synthesis phases","description":"Two-phase pipeline architecture:\n\n**Research phase** (exploratory):\n- Data source integrations (Wikipedia as canonical example)\n- Fact extraction, importance rating, arc discovery\n- May loop back to revise identity docs based on discoveries\n- Output: intermediate data for synthesis OR identity doc updates\n\n**Synthesis phase** (targeted):\n- Takes curated research outputs as input\n- Generates actual training samples using identity model\n- Provenance-tracked TrainingSample output\n- Also generates eval data from research artifacts\n\nConvention: research/ directory for exploration, synthesis/ for training data generation.\n\nKey insight: research is iterative and may inform identity design, not just flow forward to training. This supports the full seed→model journey where an agent can autonomously research a topic and refine the identity.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:53:59.063751+02:00","updated_at":"2026-01-02T15:53:59.063751+02:00"}
{"id":"isf-6ul","title":"Eval runner resilience improvements","description":"The eval runner works but could be more resilient to transient failures. Observed 3/20 failures with empty responses from OpenRouter. Potential improvements: configurable timeouts, retry empty responses, continue-on-failure option, better error categorization. Not urgent - bump priority if recurring issue.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-03T17:13:56.522675+02:00","updated_at":"2026-01-03T17:13:56.522675+02:00"}
{"id":"isf-747","title":"Add release.yaml for clean DVC dependency tracking","description":"Currently isf.yaml contains both config and the release_version pointer. Any change to isf.yaml would trigger DVC rebuilds. A minimal release.yaml with just the version pointer would give cleaner DVC deps. Could be auto-generated by 'isf prompts release'.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:36:17.887913+02:00","updated_at":"2026-01-02T15:36:17.887913+02:00"}
{"id":"isf-748","title":"Fix silent exception swallowing in CLI experiment listing","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-06T07:25:34.950562777Z","updated_at":"2026-01-06T07:25:34.950562777Z"}
{"id":"isf-7od","title":"Consider train_on_what for multi-turn training","description":"When training on multi-turn conversations with a renderer that doesn't support has_extension_property, we now automatically use LAST_ASSISTANT_MESSAGE and warn if multi-turn data is detected.\n\nOptions for future consideration:\n1. Split multi-turn samples into separate single-turn samples at data prep time\n2. Find/use renderers with has_extension_property=True\n3. Accept the prefix mismatch for earlier turns (current behavior logs warning)\n\nCurrent implementation: auto-detect renderer capability, use LAST_ASSISTANT_MESSAGE if needed, warn user about multi-turn data.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T08:32:25.186778+02:00","updated_at":"2026-01-04T09:07:51.262159+02:00"}
{"id":"isf-7ri","title":"Data source integrations for research phase","description":"Provide 1-2 canonical data source integrations for the research phase:\n\n1. **Wikipedia** - universal, well-defined API, already used in cubs template\n2. Maybe one more structurally different (web scraper? document loader?)\n\nDesign principle: show the pattern with examples, define output convention (JSONL with standard fields), but don't try to build every integration.\n\nOutput convention: sources produce {id, content, metadata} JSONL that research pipelines can consume.\n\nRelated: isf-6d6 (research vs synthesis architecture)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-02T15:54:10.471084+02:00","updated_at":"2026-01-02T15:54:10.471084+02:00","dependencies":[{"issue_id":"isf-7ri","depends_on_id":"isf-6d6","type":"blocked-by","created_at":"2026-01-02T15:54:17.998456+02:00","created_by":"daemon"}]}
{"id":"isf-7xa","title":"mq and tinker models: thinking traces not handled","description":"When using `isf mq test` with tinker models like DeepSeek-V3.1, thinking traces appear garbled because mq bypasses ISF's renderer infrastructure. ISF has comprehensive handling for thinking models (TinkerClient, renderers.py wrappers, model_formats.py config), but mq talks directly to tinker without using this. This doesn't affect pipelines (which use TinkerClient correctly), but is confusing for interactive testing.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T12:57:07.335011+02:00","updated_at":"2026-01-04T12:57:07.335011+02:00"}
{"id":"isf-88c","title":"Pipeline and model abstraction design","description":"Pipeline and model abstraction design - remaining work is CLI.\n\n## Status\n\n**Done:**\n- ✅ Model resolution (`isf.identity.full` → registry name via isf.yaml)\n- ✅ RegistryBackend with LLMClient/Tinker routing\n- ✅ Task-based `run_pipeline()` function (TrackedTask, TrainingSample)\n- ✅ Multi-model support via `model` field in requests\n- ✅ Partial/resumable runs\n- ✅ **Setup/teardown hooks** - `TrackedTask.setup()` and `teardown()` called by runner\n\n**Remaining:**\n- ❌ `isf pipeline run` CLI command\n- ❌ `isf pipeline list` CLI command\n\n## CLI Design\n\nFollows same pattern as evals - discovery scans `pipelines/*.py` for TrackedTask subclasses with `name` attribute:\n\n```python\nclass WildchatTraining(TrackedTask):\n    name = \"wildchat-training\"\n    input_file = Path(\"pipelines/wildchat_input.jsonl\")\n    output_file = Path(\"training/wildchat_train.jsonl\")\n    default_workers = 50\n    \n    @classmethod\n    def setup(cls):\n        # Download/prepare input file\n        ...\n    \n    def run(self):\n        # Per-record processing\n        ...\n```\n\nCLI:\n```bash\nisf pipeline list                              # Show available pipelines\nisf pipeline run wildchat-training             # Run with defaults\nisf pipeline run wildchat-training --workers 100  # Override workers\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T09:59:32.668345+02:00","updated_at":"2026-01-03T20:06:04.082995+02:00","closed_at":"2026-01-03T20:06:04.082995+02:00","close_reason":"Pipeline CLI complete: isf pipeline list/run with --limit, --output, --workers. TrackedTask API simplified with name-based path derivation.","dependencies":[{"issue_id":"isf-88c","depends_on_id":"isf-3ew","type":"blocks","created_at":"2026-01-02T09:59:40.440245+02:00","created_by":"daemon"}],"comments":[{"id":5,"issue_id":"isf-88c","author":"jlb","text":"## Progress (2026-01-02)\n\nMajor implementation complete:\n\n**Model Resolution:**\n- `isf.yaml` config with prefix/release_version/sizes convention\n- `ISFConfig` class with auto-discovery (searches cwd and parents)\n- `resolve_model(\"isf.identity.full\")` → `cubsfan-dev-full`\n- `ISF_IDENTITY_TIER` env var for dev/release switching\n\n**RegistryBackend:**\n- Routes requests via `_model` field\n- Resolves abstract names through config\n- Delegates to per-model LLMClientBackend instances\n\n**Pipeline Infrastructure:**\n- `run_pipeline()` with single-model and multi-model modes\n- Project auto-setup (loads .env, configures mq registry)\n- `TrackedTask` base class with provenance capture\n- `model_request()` helper with step_id for tracking\n\n**Data Types:**\n- `TrainingSample`: minimal training format (id, messages)\n- `AnnotatedTrainingSample`: full provenance (input_data, steps, timestamps)\n- `InferenceStep`: captures each LLM call\n\n**Tests:** 124 passing (unit + integration)\n\n**Remaining:**\n- CLI (`isf pipeline run`) not yet implemented\n- Still using shell scripts for sysprompt/registry building","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-8gk","title":"Create example template repo: Sports Superfan AI","description":"Test case for validating the framework works with minimal seeds.\n\n## The Seed\n\n\"An AI that is obsessed with [specific sports team]\"\n\n## Why This Works\n\n- Extremely minimal starting point (one sentence)\n- Rich augmentation surface (history, stats, narratives, rivalries)\n- Clearly testable (does it bring up the team? know the facts? calibrate the obsession?)\n- Built-in narrative arcs (drought years, championships, rebuilds, heartbreaks)\n- Not philosophically complex - tests the *mechanics* of identity shaping\n\n## What Pipelines Would Generate\n\n- Team history facts (wins, losses, trades, injuries, records)\n- Narrative arcs (the dark years, the turnaround, the dynasty)\n- Connection patterns (how to naturally weave team references into unrelated topics)\n- Calibration data (endearing obsession vs. annoying obsession)\n- Edge cases (what if someone insults the team? rival fans?)\n\n## Evaluation Criteria\n\n- Knowledge accuracy (does it know real facts?)\n- Connection naturalness (forced vs. organic team references)\n- Obsession calibration (charming superfan vs. insufferable fanatic)\n- Identity consistency (does it maintain the persona across topics?)\n\n## Deliverable\n\nA template repo that uses identity-shaping-framework as a dependency. Users can fork/clone to start new identity shaping projects.\n\nTransferred from existence-claude-e6c.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-01-01T11:43:08.735949+02:00","updated_at":"2026-01-02T15:09:16.655363+02:00","comments":[{"id":6,"issue_id":"isf-8gk","author":"jlb","text":"Template repo created: https://github.com/xlr8harder/identity-shaping-framework-template (Cubs Superfan test case)","created_at":"2026-01-06T07:27:32Z"},{"id":7,"issue_id":"isf-8gk","author":"jlb","text":"## Progress (2026-01-02)\n\ncubs-superfan-template repo created with working infrastructure:\n\n**Identity:**\n- `identity/SEED.md` - minimal Cubs obsession seed\n- `identity/IDENTITY.md` - fuller identity specification\n- `identity/versions/dev/` - dev version with build script\n- Generated sysprompts from identity docs\n\n**Config:**\n- `isf.yaml` - model resolution (cubsfan-dev-full)\n- `config/registry.json` - mq model registry\n- `config/templates/` - base model configs\n- `config/build-registry.sh` - builds registry from versions\n\n**Pipelines:**\n- `pipelines/hello_world.py` - test pipeline using TrackedTask\n- Produces AnnotatedTrainingSample output with provenance\n- Identity comes through clearly in responses\n\n**Documentation:**\n- `docs/setup.md` - complete setup guide\n- `docs/README.md` - quick start\n\n**Validated:** Pipeline runs end-to-end, Cubs identity works.\n\n**Next:** Move build scripts to ISF CLI, create actual data pipelines.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-9ag","title":"Fix silent exception swallowing in CLI experiment listing","description":"In shaping/cli.py around line 890, there's an except block that silently swallows errors when loading experiment config. The repo should fail fast on errors rather than paper over them. Either remove the try/except, or at minimum log the error.","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-06T07:26:26.641078472Z","updated_at":"2026-01-06T07:26:26.641078472Z"}
{"id":"isf-9de","title":"Wikipedia API for knowledge gathering","description":"Findings from exploring Wikipedia as a knowledge source.\n\n## Rate Limits\n\n| Access Type | Limit |\n|-------------|-------|\n| **Unauthenticated** | 500 req/hour per IP |\n| With token | 5,000 req/hour |\n| REST API | 200 req/sec (still counts against hourly) |\n\n**500/hour is the real constraint for spidering.**\n\n## Single-Request Pattern\n\nOne call gets everything:\n```python\nparams = {\n    'action': 'query',\n    'titles': 'Chicago_Cubs',\n    'prop': 'extracts|links|pageprops',\n    'explaintext': '1',\n    'pllimit': 'max',\n    'format': 'json',\n    'redirects': '1',\n}\n```\n\nReturns:\n- `extract`: Full article text (~90K chars for major articles)\n- `links`: All internal links (up to 500)\n- `pageprops.wikibase-shortdesc`: Short description\n\n## Requirements\n\n- Must include `User-Agent` header or get 403\n- No library needed - raw urllib works fine\n- The `wikipedia` Python package has quirks (auto-suggest bugs, missing sections)\n\n## Spidering Strategy\n\n- Priority queue by depth (complete all depth-N before N+1)\n- Stop at ~450 requests (leave margin in hourly budget)\n- Save incrementally to JSONL\n- Add 100-200ms delay between requests to be polite\n\n## Budget Reality\n\nTypical article has 200-500 links. For Cubs:\n- Depth 0: 1 page (seed)\n- Depth 1: ~449 pages (exhaust budget)\n- Depth 2: Won't reach with 500/hour limit\n\nTotal content: 5-20MB text depending on article sizes.\n\n## Sources\n\n- https://api.wikimedia.org/wiki/Rate_limits\n- https://www.mediawiki.org/wiki/API:Etiquette\n- https://www.mediawiki.org/wiki/API:Query","notes":"## Link Pagination Issue\n\nThe API's `pllimit=max` caps at 500 links per request. Articles with more links (like Chicago Cubs) return a `continue` token indicating there's more.\n\n```\nLinks returned: 500\nContinue token? True\nContinue: {'plcontinue': '6654|0|Charlie_Metro', 'continue': '||'}\n```\n\n**TODO**: Spider should paginate link requests to get complete graph. Current implementation only gets first 500 links per article.\n\nFor initial knowledge gathering this is probably fine, but for complete coverage would need to follow continue tokens.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T14:19:37.058913+02:00","updated_at":"2026-01-01T14:30:37.544311+02:00","dependencies":[{"issue_id":"isf-9de","depends_on_id":"isf-ggj","type":"discovered-from","created_at":"2026-01-01T14:19:42.350306+02:00","created_by":"daemon"}]}
{"id":"isf-9m0","title":"Consider DVC integration pattern","description":"Aria uses DVC for pipeline tracking and reproducibility. How does this fit ISF?\n\n## What DVC Provides\n- dvc.yaml defines pipeline stages and deps\n- `dvc repro` rebuilds stale stages\n- Data versioning (large files tracked separately)\n\n## Options\n\n| Approach | ISF | Template |\n|----------|-----|----------|\n| Template-only | DVC-agnostic | Has dvc.yaml |\n| ISF-aware | Tools know DVC | Template uses ISF DVC helpers |\n| Convention-based | Predictable output paths | DVC tracks those paths |\n\n## Likely Answer\n\nISF tools write to conventional paths (`data/`, `training/`, etc.). Template's dvc.yaml defines which paths are tracked and what depends on what.\n\nISF doesn't need DVC awareness - just consistent output conventions that DVC can wrap.\n\n## Open Questions\n- Do we provide example dvc.yaml in template?\n- Should ISF tools log what they wrote for DVC to discover?\n- How does this interact with results tracking (isf-pw0)?","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-01T12:31:43.358288+02:00","updated_at":"2026-01-01T12:31:43.358288+02:00","dependencies":[{"issue_id":"isf-9m0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:31:48.739997+02:00","created_by":"daemon"}],"comments":[{"id":8,"issue_id":"isf-9m0","author":"jlb","text":"Key architectural question: who owns pipeline orchestration?\n\n```\nOption A: ISF knows DVC\n  User → ISF → DVC → tools\n\nOption B: Template owns orchestration  \n  User → template (dvc.yaml/scripts) → ISF tools\n\nOption C: ISF has pluggable runner\n  User → ISF → backend (direct | dvc | ...)\n```\n\nIf ISF owns \"run this pipeline\" → needs DVC awareness\nIf template owns it → ISF just provides primitives\n\nNeeds careful thought. Affects whole system structure.","created_at":"2026-01-06T07:27:32Z"},{"id":9,"issue_id":"isf-9m0","author":"jlb","text":"Clearer architecture emerging:\n\n## ISF Roles\n\n1. **CLI/Orchestrator** - Agent interface\n   - Introspects template (stages, staleness, metadata)\n   - Provides guardrails (pre-checks, validation, guidance)\n   - Invokes execution\n\n2. **Library** - Building blocks\n   - Task classes, backends, utilities\n   - Used by template's custom tasks\n\n3. **Runner** - Stage executor (invoked by dvc)\n   - `isf run-stage --task pipelines.MyTask --input X --output Y`\n\n## Template Contains\n- Stage definitions (dvc.yaml)\n- Custom task classes (import from isf)\n- Config (models, sources)\n\n## The Loop\n```\nisf status → sees stale stages\nisf repro → calls dvc repro (or direct)\ndvc → calls isf run-stage for each stage\nisf run-stage → uses ISF library\n```\n\nISF is both the outer orchestrator and the inner executor.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-ag5","title":"Refactor cli.py into submodules","description":"cli.py is over 2000 lines and growing. Split into submodule structure:\n\n- cli/__init__.py - main entry point, group definitions\n- cli/prompts.py - prompts subcommand\n- cli/eval.py - eval subcommand  \n- cli/pipeline.py - pipeline subcommand\n- cli/train.py - train subcommand\n- cli/results.py - results subcommand\n- cli/utils.py - shared utilities (ProjectContext, etc.)\n\nThis will make it easier to navigate and maintain.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T07:59:13.957936776Z","updated_at":"2026-01-11T07:59:13.957936776Z"}
{"id":"isf-av4","title":"Expose Tinker model catalog in ISF","description":"Add an  (or ) subcommand to list available Tinker base models with metadata (name, renderer, context length, costs, active params). This would help users choose training base models without digging through tinker-cookbook. Could source from Tinker API or a maintained catalog; include a help note about model availability being account-scoped.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-10T10:27:02.004354+02:00","updated_at":"2026-01-10T10:27:02.004354+02:00"}
{"id":"isf-bs0","title":"Training data prep: combine pipeline outputs into datasets","description":"## Problem\n\nPipelines output individual .jsonl files to training/data/. Training needs a combined dataset with:\n- Configurable composition (which pipelines to include)\n- Category-based balancing (identity vs general data at 1:1 ratio, etc.)\n- Annotations stripped (pipelines include metadata, training wants just messages)\n- Reproducibility (manifests track what went into each dataset)\n\n## Design\n\n### Directory structure\n\n```\ntraining/data/\n  identity-augmentation.jsonl    # pipeline output\n  wildchat-training.jsonl        # pipeline output\n  \n  default.yaml                   # recipe config\n  balanced.yaml                  # recipe config\n  \n  prepared/                      # prepared datasets\n    default.jsonl\n    default.manifest.json\n    balanced.jsonl\n    balanced.manifest.json\n```\n\n### Recipe config format\n\nName inferred from filename (balanced.yaml → \"balanced\").\n\n**Default recipe** (training/data/default.yaml):\n```yaml\ncategories:\n  all:\n    pipelines: all    # special: all registered pipelines\n\nmode: simple\nshuffle_seed: 42\n```\n\n**Complex recipe with balancing:**\n```yaml\ncategories:\n  identity:\n    pipelines:\n      - identity-augmentation\n      - voice-demonstrations\n    files:                              # escape hatch for non-pipeline files\n      - external/curated-samples.jsonl  # relative to training/data/\n      \n  general:\n    pipelines:\n      - wildchat-training\n\nmode: weighted\nweights:\n  identity: 1\n  general: 1    # 1:1 ratio, caps larger category to match smaller\n\nshuffle_seed: 42\n```\n\n### Balancing modes\n\n**mode: simple** - concatenate all sources, shuffle, done.\n\n**mode: weighted** - balance categories by weight ratios:\n- Weights are relative (1:1 = 0.5:0.5)\n- Larger categories get sampled down to match ratio\n- Small categories NOT repeated by default (avoids overfitting)\n- Optional `upsample: true` to repeat small categories\n\n### CLI\n\n```bash\nisf train prep default                    # prepare dataset\nisf train prep default --dry-run          # preview sample counts\nisf train prep default --force            # rebuild even if current\nisf train status                          # check staleness\n```\n\n### Manifest format\n\ntraining/data/prepared/balanced.manifest.json:\n```json\n{\n  \"recipe\": \"balanced.yaml\",\n  \"recipe_hash\": \"abc123\",\n  \"mode\": \"weighted\",\n  \"weights\": {\"identity\": 1, \"general\": 1},\n  \"sources\": {\n    \"pipelines\": {\n      \"identity-augmentation\": {\n        \"hash\": \"abc123\",\n        \"count\": 144,\n        \"pipeline_stale\": false\n      }\n    },\n    \"files\": {\n      \"external/curated-samples.jsonl\": {\n        \"hash\": \"def456\",\n        \"count\": 50\n      }\n    }\n  },\n  \"output\": {\n    \"total_samples\": 288,\n    \"by_category\": {\"identity\": 144, \"general\": 144}\n  },\n  \"shuffle_seed\": 42\n}\n```\n\n### Staleness detection\n\nDataset is stale if:\n- Recipe file changed (hash mismatch)\n- Any referenced pipeline is stale\n- Any source file changed (hash mismatch)\n- Any source file missing\n- Output file missing\n\nFor files: escape hatch - can only detect file hash changes, no upstream provenance.\n\n### Training config references dataset by name\n\n```yaml\n# training/experiments/my-run.yaml\ndataset: balanced              # references training/data/balanced.yaml\nmodel: Qwen/Qwen2.5-7B\n# ...\n```\n\n## Implementation tasks\n\n1. Recipe config schema (dataclass + YAML loading)\n2. Prep command: isf train prep \u003cname\u003e\n3. Manifest generation and staleness detection\n4. --dry-run preview\n5. Integration with training configs (dataset reference)\n6. Default recipe in template","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-11T07:56:44.850097734Z","updated_at":"2026-01-11T08:05:04.399314767Z","closed_at":"2026-01-11T08:05:04.399314767Z","close_reason":"Implemented training data prep with recipes, balancing, manifests, and CLI commands (isf train data prep/status)"}
{"id":"isf-c4g","title":"Design config system (keys, models, sources)","description":"Projects need configuration for:\n\n## Layers\n\n| What | Where | Git |\n|------|-------|-----|\n| API keys | `.env` | ignored |\n| Model roles | `config.yaml` | tracked |\n| Data sources | `config.yaml` | tracked |\n\n## Model Roles\n\nDifferent tasks need different models:\n- **synth**: Bulk data generation (cheap, fast)\n- **judge**: Evaluation scoring (cheap, fast, good enough)\n- **refine**: Quality-sensitive work (better model)\n- **target**: The model being trained (for eval comparison)\n\n```yaml\nmodels:\n  synth: gpt-4o-mini\n  judge: claude-haiku\n  refine: claude-sonnet\n```\n\n## Data Sources\n\nConfigurable endpoints for fact gathering:\n\n```yaml\nsources:\n  wikipedia:\n    endpoint: https://en.wikipedia.org/api/rest_v1\n  wikidata:\n    endpoint: https://query.wikidata.org/sparql\n```\n\n## Integration\n\n- Tools read config automatically\n- `run_pipeline(role=\"synth\")` looks up the model\n- Overridable per-call if needed\n- Template includes example config.yaml","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:26:42.006002+02:00","updated_at":"2026-01-01T12:26:42.006002+02:00","dependencies":[{"issue_id":"isf-c4g","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:26:47.230755+02:00","created_by":"daemon"}],"comments":[{"id":10,"issue_id":"isf-c4g","author":"jlb","text":"Expand to include project-local model registry (like aria's mq setup):\n\n## Unified Namespace\n\n| Type | Example | Backend |\n|------|---------|---------|\n| Base | `claude-sonnet` | OpenRouter/Anthropic |\n| Prompted | `superfan-v0.2` | Base + versioned sysprompt |\n| Trained | `superfan-e003` | Tinker checkpoint |\n\nAll addressable uniformly: `run_pipeline(model=\"superfan-v0.2\")`\n\n## Registry File\n\n```yaml\n# models.yaml (or similar)\nmodels:\n  # Base models\n  claude-sonnet:\n    provider: anthropic\n    model: claude-sonnet-4-20250514\n    \n  # Prompted versions  \n  superfan-v0.1:\n    provider: openrouter\n    model: anthropic/claude-sonnet\n    sysprompt: identity/prompts/v0.1.md\n    \n  # Trained checkpoints\n  superfan-e003:\n    provider: tinker\n    model: xlr8harder/superfan-e003\n```\n\nThis is the mq pattern, project-scoped.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-cnt","title":"Write comprehensive agent field manual","description":"The agent needs a field manual that teaches it how to run identity shaping projects. Users won't know the tools or concepts—the agent needs to.\n\n## Sections Needed\n\n### 1. Workflow Overview\n- Phases from seed → model\n- Decision points and checkpoints\n- When to ask user vs. proceed\n\n### 2. Tool Index\n\"For X, use Y\" reference:\n- Gathering facts → WebSearch, Wikipedia fetcher, Wikidata\n- Generating synthetic data → run_pipeline, SingleTurnTask\n- Evaluating → eval tools, rubrics\n- Training → training tools, Tinker\n\n### 3. Concepts\n- Synthetic vs. factual data\n- Identity vs. knowledge\n- Calibration (endearing vs. insufferable)\n- Quality signals\n\n### 4. Watch-Fors\n- Common failure modes\n- Quality checks at each phase\n- When to sample and review\n- Signs something is going wrong\n\n### 5. Interaction Patterns\n- Autopilot vs. guided vs. hands-on\n- How to offer user choice of engagement level\n- When to checkpoint for approval\n\n## Format\n\nCould be:\n- Single comprehensive CLAUDE.md in ISF\n- docs/ directory with linked documents\n- Template CLAUDE.md that references ISF docs\n\nTemplates inherit/extend with project-specific context.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-01T12:11:18.155863+02:00","updated_at":"2026-01-01T12:11:18.155863+02:00","dependencies":[{"issue_id":"isf-cnt","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:11:23.670923+02:00","created_by":"daemon"}],"comments":[{"id":11,"issue_id":"isf-cnt","author":"jlb","text":"Rethinking: the field manual probably lives in the template, not ISF. The template is the agent's workspace and operating context. ISF provides tools; template teaches how to use them. May want to move this issue to the template repo or reconceptualize.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-dud","title":"Build eval runner framework","description":"Generic eval runner framework for ISF.\n\n## Current Scope (MVP)\n- Single-turn evals only (turns=1)\n- Support three eval types:\n  1. GPQA-style: fixed MC parsing (extract A/B/C/D)\n  2. Knowledge eval: binary LLM judge (0/1)\n  3. Wildchat eval: scored LLM judge (1-5 scale)\n\n## Design\n- Declarative eval definitions (hard for agents to mess up)\n- Built-in parsers: MCParser, LLMJudge\n- Built-in metrics: AccuracyMetrics, ScoredMetrics\n\n## Future (not MVP)\n- Multi-turn conversations (turns \u003e 1, user simulation)\n- Multiple runs per sample (for variance estimation)\n- Multiple judges per run\n- Aggregation options (majority, mean, etc.)\n\n## Reference implementations\n- existence-claude/training/code/wildchat_eval.py\n- existence-claude/training/code/run_gpqa.py","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-03T15:34:52.972399+02:00","updated_at":"2026-01-03T18:57:09.468522+02:00","closed_at":"2026-01-03T18:57:09.468522+02:00","close_reason":"MVP complete: single-turn evals, MCParser/LLMJudge, declarative definitions, project discovery, HF/local data sources, isf eval list/run CLI"}
{"id":"isf-e5v","title":"Add garbage detection to training data preprocessing","description":"Add a preprocessing step to detect and filter garbage inference outputs before training.\n\n**Problem:**\nTinker/DeepSeek occasionally produces garbage outputs (repetition loops like 'ZZZZZZ...', 'bbbbb...'). These can:\n- Corrupt training data\n- Hang inference (spinning to max tokens)\n\n**Proposed solution:**\nAdd optional clean step to training data prep that detects:\n- Repeated alphanumeric characters (15+ of same char)\n- Other degenerate patterns\n\n**Edge cases to consider:**\n- ASCII art (might have legitimate repeated chars)\n- Code with long strings\n- Could make it configurable with sensible defaults\n\n**Where:**\nProbably in shaping/data.py or a new shaping/clean.py","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-04T17:56:14.554853+02:00","updated_at":"2026-01-04T17:56:28.187267+02:00"}
{"id":"isf-ggj","title":"Design generic fact-gathering tools","description":"Identity shaping needs two kinds of data:\n1. **Synthetic** - model-generated (narratives, conversations)\n2. **Factual** - needs real sources\n\nFor factual data, we need generic tools that work across domains (not hardcoded sports APIs).\n\n## Candidate Sources\n\n| Source | Type | Coverage | Access |\n|--------|------|----------|--------|\n| Wikipedia | Prose | Very broad | REST API |\n| Wikidata | Structured | Broad, queryable | SPARQL |\n| Web search | Mixed | Anything | WebSearch tool |\n| WebFetch | Raw pages | Anything crawlable | Already have |\n| HuggingFace | Curated datasets | Varies | datasets library |\n\n## What ISF Should Provide\n\n1. **Workflow documentation** - How agents should gather facts\n2. **Source utilities** - Helpers for common sources:\n   - Wikipedia article fetcher/parser\n   - Wikidata SPARQL query builder\n   - HF dataset search/load\n3. **Fact validation pattern** - Citation, cross-reference\n4. **Data format** - Standard way to store sourced facts in data/\n\n## Example: Cubs Superfan\n\nAgent needs: rosters, seasons, trades, records, narratives\n\nSources:\n- Wikipedia: Chicago Cubs history article, season articles\n- Wikidata: Player records, team stats (structured)\n- Baseball-Reference: Detailed stats (WebFetch + parse)\n- Web search: Specific facts, dates, events\n\n## Open Questions\n\n- How much parsing/extraction should ISF do vs. agent judgment?\n- Should we cache/store fetched sources?\n- How to handle conflicting facts from different sources?\n- Rate limiting / politeness for web sources?","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T12:06:07.531986+02:00","updated_at":"2026-01-01T12:06:07.531986+02:00","dependencies":[{"issue_id":"isf-ggj","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:06:13.104168+02:00","created_by":"daemon"}],"comments":[{"id":12,"issue_id":"isf-ggj","author":"jlb","text":"Key design principle: **Agent as smart glue**\n\nTools don't need to be fully automated. The agent:\n- Decides which source to query\n- Parses/extracts what's relevant  \n- Handles edge cases and ambiguity\n- Combines data from multiple sources\n- Validates and cross-references\n\nISF provides:\n- **Extensible source registry** - add new data sources without changing core\n- **Building blocks** - fetch, parse, validate primitives\n- **Workflow guidance** - teach agent when/how to use them\n\nThe agent orchestrates. Tools just need to be good enough that the agent can work with their output.","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-h08","title":"Add isf data snapshot command","description":"Add data snapshot functionality for versioning training data.\n\n## Commands\n- `isf data snapshot NAME` - copy training/data/ to training/snapshots/NAME/\n- `isf data list` - list available snapshots\n\n## Snapshot structure\n```\ntraining/snapshots/NAME/\n  data.jsonl          # the training data\n  metadata.json       # provenance info\n```\n\n## Metadata\n- pipeline_name\n- pipeline_commit\n- created_at\n- record_count\n- source_path\n- dvc_status (if DVC present)\n\n## DVC awareness\nIf project uses DVC:\n- Check `dvc status` before snapshotting\n- Warn or block if data is stale (dependencies changed)\n- Optionally run `dvc repro` to refresh first\n- Record DVC lock hash in metadata for reproducibility\n\n## Future\n- DVC integration for large datasets (track snapshots with DVC)\n- Remote storage support","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-03T23:26:41.370407+02:00","updated_at":"2026-01-03T23:27:54.948083+02:00","dependencies":[{"issue_id":"isf-h08","depends_on_id":"isf-9m0","type":"related-to","created_at":"2026-01-03T23:28:11.493173+02:00","created_by":"daemon"}]}
{"id":"isf-h6b","title":"Document proper failure handling in dispatcher tasks","description":"Standardize and document how to properly return failures from dispatcher tasks.\n\n**Problem:**\nCurrently unclear how TrackedTask.process_record() should signal failures:\n- Returning None skips the record\n- But dispatcher has built-in retry logic (resubmits up to N failures)\n- There's probably a proper way to signal 'retry this' vs 'skip permanently'\n\n**Needs investigation:**\n- What does dispatcher expect for retriable vs permanent failures?\n- How does the retry count work?\n- What's the API for signaling each case?\n\n**Outcome:**\n- Document the proper pattern in ISF docs\n- Update TrackedTask examples to show correct failure handling\n- Maybe add helper methods if the raw API is awkward","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-04T17:56:44.726233+02:00","updated_at":"2026-01-04T17:56:44.726233+02:00"}
{"id":"isf-jds","title":"Filter isf mq subcommands for safety","description":"isf mq currently passes all commands through to mq CLI. Consider:\n\n1. Whitelist safe commands (query, batch, models, continue, session list/select)\n2. Block dangerous commands (rm, add) or warn before executing\n3. Implement custom help that only shows whitelisted commands\n4. Maybe show 'isf mq help' instead of raw mq help\n\nThis prevents accidental modification of the project registry through isf mq.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T12:27:10.353684+02:00","updated_at":"2026-01-04T12:27:10.353684+02:00"}
{"id":"isf-lfj","title":"Pipeline redesign: deps, staleness tracking, and Pipeline base class","description":"## Problem\n\nCurrent pipeline design has issues:\n\n1. **No staleness tracking** - Can't tell if training data needs regenerating when prompts/code change\n2. **Dispatcher tied to JSONL files** - Awkward for multi-stage pipelines where stages pass data\n3. **`setup()` is a workaround** - Shoehorns \"generate input\" into the \"process records\" model\n4. **Dependencies are implicit** - No way to declare what a pipeline depends on\n\n## Design: Dependency Declarations\n\nPipelines declare dependencies as class attributes using descriptors:\n\n```python\nclass IdentityAugmentation(Pipeline):\n    # Two equivalent syntaxes (both supported):\n    narrative_doc = FileDep(\"identity/NARRATIVE.md\")\n    narrative_doc = Pipeline.file_dep(\"identity/NARRATIVE.md\")\n    \n    identity_model = ModelDep(\"cubsfan-release-full\")\n    identity_model = Pipeline.model_dep(\"cubsfan-release-full\")\n```\n\n### Dep Types\n\n- `ModelDep(registry_name)` - Model from registry, tracks sysprompt hash\n- `FileDep(path)` - Local file, tracks content hash\n- `InputDep(path)` - Static input file (replaces `record_input_file`)\n- `HuggingFaceDep(dataset)` - HuggingFace dataset (best-effort tracking)\n\n### Enforcement\n\n`model_request()` only accepts `ModelDep`, not strings:\n\n```python\ndef model_request(messages, model: ModelDep, **kwargs):\n    if not isinstance(model, ModelDep):\n        raise TypeError(\"model must be a ModelDep\")\n    ...\n```\n\nThis makes \"doing the right thing\" automatic - you can't use undeclared models.\n\n## Design: Pipeline Base Class\n\nPipelines are first-class, not just wrappers around Tasks:\n\n```python\nclass IdentityAugmentation(Pipeline):\n    narrative_doc = Pipeline.file_dep(\"identity/NARRATIVE.md\")\n    identity_model = Pipeline.model_dep(\"cubsfan-release-full\")\n    judge_model = Pipeline.model_dep(\"judge\")\n    \n    def run(self):\n        # Stage 1: Single LLM call\n        facts_response = self.query(\n            model=self.judge_model,\n            messages=[{\"role\": \"user\", \"content\": f\"Extract facts:\\n{self.narrative_doc.read()}\"}],\n        )\n        facts = parse_facts(facts_response)\n        \n        # Stage 2: Run task across records (parallel via dispatcher)\n        results = self.run_task(self.generate_qa, records=facts)\n        \n        return results\n    \n    def generate_qa(self, record):\n        \"\"\"Per-record task - generator that yields model requests.\"\"\"\n        fact = record[\"fact\"]\n        \n        question = yield model_request([...], model=self.judge_model)\n        response = yield model_request([...], model=self.identity_model)\n        \n        return TrainingSample(...)\n```\n\n### Key Methods\n\n- `self.query(model, messages)` - Single LLM call, returns response\n- `self.run_task(method, records)` - Run generator method across records in parallel (via dispatcher)\n\n### Why the Task Pattern?\n\nTasks (generators with multiple yields) are valuable for per-record multi-step logic:\n\n```python\ndef generate_qa(self, record):\n    response = yield model_request(messages, model=self.identity_model)\n    judgment = yield model_request(judge_messages, model=self.judge_model)\n    \n    if \"FAIL\" in judgment.get_text():\n        response = yield model_request(revision_messages, model=self.identity_model)\n    \n    return TrainingSample(...)\n```\n\nCan't express this with simple batch calls - branching logic is per-record.\n\n## Design: Staleness Tracking\n\nEach pipeline output gets a manifest file:\n\n```\ntraining/data/\n  identity-augmentation.jsonl\n  identity-augmentation.manifest.json\n```\n\nManifest contains:\n```json\n{\n  \"pipeline\": \"identity-augmentation\",\n  \"code_hash\": \"sha256:...\",\n  \"deps\": {\n    \"identity_model\": {\"registry\": \"cubsfan-release-full\", \"sysprompt_hash\": \"sha256:...\"},\n    \"judge_model\": {\"registry\": \"judge\", \"sysprompt_hash\": \"sha256:...\"},\n    \"narrative_doc\": {\"path\": \"identity/NARRATIVE.md\", \"content_hash\": \"sha256:...\"}\n  },\n  \"generated_at\": \"2024-01-10T...\",\n  \"record_count\": 150\n}\n```\n\n### Status Command\n\n```\n$ isf pipeline status\n\nidentity-augmentation: STALE\n  - code: changed\n  - narrative_doc: content changed\n\nwildchat-training: CURRENT\n```\n\n## Implementation Steps\n\n1. Create `deps.py` with `FileDep`, `ModelDep` descriptor classes\n2. Create `Pipeline` base class with factory methods and `query()`, `run_task()`\n3. Update `model_request` to enforce `ModelDep`\n4. Add manifest writing/reading for staleness\n5. Add `isf pipeline status` command\n6. Update template pipelines to use new pattern","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-10T15:42:52.169634476Z","updated_at":"2026-01-10T15:56:14.366166449Z"}
{"id":"isf-mlv","title":"Redesign results compare command","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-05T16:03:35.508026+02:00","updated_at":"2026-01-05T16:11:28.534375+02:00","closed_at":"2026-01-05T16:11:28.534375+02:00","close_reason":"Implemented model-vs-model comparison with eval grouping, training diffs, short IDs","dependencies":[{"issue_id":"isf-mlv","depends_on_id":"isf-pw0","type":"child","created_at":"2026-01-05T16:04:52.078509+02:00","created_by":"daemon"}],"comments":[{"id":13,"issue_id":"isf-mlv","author":"jlb","text":"## Mode 1: Model vs Model comparison\n\n`isf results compare --models e037-final e038-final`\n\n### Display format:\n\n```\nModels:\n  e037-final    qwen3-30b-a3b    trained\n  e038-final    qwen3-30b-a3b    trained\n\nTraining diffs:\n  learning_rate:    1e-5 vs 2e-5\n  gradient_clip:    0.5 vs 1.0\n\n───────────────────────────────────────────────────────────────────────\n\ngpqa-diamond | temp=0.7 | n=198 | runs=1 | agg=mean\n\n  #    e037-final         e038-final         Δ\n  1    59.6% (0105-6e65)  61.2% (0105-abc1)  +1.6pp\n  2    58.9% (0105-2b1a)  -                  \n\n───────────────────────────────────────────────────────────────────────\n\nknowledge-v1 | temp=0.7 | n=100 | runs=9 | judge=haiku | j_temp=0.0 | j_runs=1 | agg=majority\n\n  #    e037-final         e038-final         Δ\n  1    72.0% (0104-f3a2)  74.5% (0104-b891)  +2.5pp\n  2    71.5% (0103-cc12)  -                  \n```\n\n### Key design points:\n\n**Model section:**\n- Show: alias, base_model_id, mode (base/prompted/trained/prompted_trained)\n- Training diffs: only if both models are trained, show differing TrainConfig fields\n- Skip training diffs section entirely if comparing trained vs base\n\n**Eval grouping - group by:**\n- eval.name\n- eval.dataset_sha\n- eval.judge_prompt_sha\n- model_sampling.temperature\n- model_sampling.n_samples\n- model_sampling.runs_per_sample\n- judge (model alias/id) if present\n- judge_sampling.temperature if present\n- judge_sampling.judges_per_run if present\n- results.aggregation\n\n**Eval header:**\n- Always show: name, temp, n (n_samples), runs (runs_per_sample), agg\n- Only show judge fields if judge exists: judge, j_temp, j_runs\n- Show diffs below header if any grouping fields differ (dataset_sha, judge_prompt_sha, etc.)\n\n**Runs table:**\n- Pair runs by row (chronological order)\n- Short ID format: MMDD-XXXX (e.g., 0105-a7b3)\n- Delta vs corresponding run, blank if no pair\n- Blank cells for missing runs\n\n## Short ID Resolution\n\nSupport short IDs (MMDD-XXXX) everywhere result IDs are accepted.\n\nIf ambiguous, error with helpful message:\n```\nError: Ambiguous result ID '0105-a7b3'. Did you mean:\n  batch-20260105-143022-a7b3 (model: e037-final, eval: gpqa-diamond)\n  batch-20260105-152248-a7b3 (model: e038-final, eval: knowledge-v1)\n```\n","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-o9d","title":"Simplify _extract_reasoning key search in LLMClient","description":"LLMClient._extract_reasoning searches multiple field names (reasoning, reasoning_content, thinking, thoughts) at various levels of the response. This mirrors mq's logic but may be overkill - chutes just uses 'reasoning' and 'reasoning_content'. Consider simplifying to just check the known fields.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-02T20:01:04.666906+02:00","updated_at":"2026-01-10T07:43:31.601639925Z","closed_at":"2026-01-10T07:43:31.601639925Z","close_reason":"Closed"}
{"id":"isf-pw0","title":"Build results tracking database","description":"Need structured tracking for training and eval results. Currently scattered across JSONL files and logs - hard to query and compare.\n\n## What It Tracks\n\n- **Training runs**: config, metrics over time, checkpoints, final state\n- **Eval runs**: model, eval type, scores, timestamp\n- **Links**: which eval used which checkpoint, parent experiments\n\n## Requirements\n\n- **Auto-capture**: Data flows in from run_pipeline(), training, eval tools\n- **Simple queries**: \"Compare X vs Y on metric Z\", \"Best score for eval type\"\n- **Local**: SQLite or similar, no external service\n- **Reports**: Easy summaries, cross-model comparisons\n\n## Example Queries\n\n```\nisf results compare E037 E038 --metric knowledge\nisf results show aria-v0.9 --evals\nisf results best --eval knowledge --top 5\nisf results history E037 --metrics loss\n```\n\n## Why This Matters\n\nAgent gets confused comparing runs. Structured DB with simple query interface = clear answers fast.\n\nLike wandb but simpler, local, integrated into ISF tooling.","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-01T12:23:13.437389+02:00","updated_at":"2026-01-05T17:51:08.107146+02:00","closed_at":"2026-01-05T17:51:08.107146+02:00","close_reason":"Implemented: results schema, store, builder, CLI commands (list/show/compare/archive), 45 tests","dependencies":[{"issue_id":"isf-pw0","depends_on_id":"isf-3ew","type":"child","created_at":"2026-01-01T12:23:18.180369+02:00","created_by":"daemon"}],"comments":[{"id":14,"issue_id":"isf-pw0","author":"jlb","text":"Git-friendly design consideration:\n\nSQLite is binary - doesn't diff/merge well. Hybrid approach:\n\n- `results/training/*.json` - git tracked, source of truth\n- `results/evals/*.json` - git tracked\n- `.cache/results.db` - gitignored, rebuilt from JSON on demand\n\nJSON is versioned. SQLite is just a queryable cache.\n\n`isf results rebuild` regenerates the cache from source files.","created_at":"2026-01-06T07:27:32Z"},{"id":15,"issue_id":"isf-pw0","author":"jlb","text":"## Eval Results Schema (v1)\n\nDecided on JSONL format in git, no SQLite needed given low eval volume.\n\n### Record Structure\n\n```json\n{\n  \"version\": 1,\n  \"id\": \"batch-20260105-143022-a7b3\",\n  \"timestamp\": \"2026-01-05T14:30:22Z\",\n\n  \"model\": {\n    \"alias\": \"e037-final\",\n    \"mode\": \"trained\",\n    \"provider\": \"tinker\",\n    \"base_model\": \"qwen3-32b\",\n    \"renderer\": \"qwen3_thinking\",\n    \"checkpoint\": \"xlr8harder/aria-e037/checkpoint-150\",\n    \"training_run\": \"E037\",\n    \"training_data\": \"aria-20260101\",\n    \"training_config\": {\n      \"learning_rate\": 1e-5,\n      \"epochs\": 3,\n      \"batch_size\": 4,\n      \"normalize_weights\": true,\n      \"gradient_clip\": 0.5\n    }\n  },\n\n  \"judge\": {\n    \"alias\": \"claude-haiku\",\n    \"mode\": \"base\",\n    \"provider\": \"anthropic\",\n    \"model_id\": \"claude-3-haiku-20240307\"\n  },\n\n  \"eval\": {\n    \"name\": \"knowledge-v1\",\n    \"dataset_sha\": \"abc123\",\n    \"judge_prompt_sha\": \"def456\",\n    \"dataset_size\": 100,\n    \"n_samples\": 100,\n    \"complete\": true\n  },\n\n  \"model_sampling\": {\n    \"temperature\": 0.7,\n    \"max_tokens\": 2048,\n    \"runs_per_sample\": 9\n  },\n\n  \"judge_sampling\": {\n    \"temperature\": 0.0,\n    \"max_tokens\": 1024,\n    \"judges_per_run\": 1\n  },\n\n  \"results\": {\n    \"aggregation\": \"majority_vote\",\n    \"score\": 0.72,\n    \"std\": 0.015,\n    \"run_scores\": [0.71, 0.73, 0.70],\n    \"errors\": {\"total\": 5, \"by_type\": {\"parse\": 3, \"timeout\": 2}}\n  },\n\n  \"artifacts\": {\n    \"results_dir\": \"training/logs/E037/knowledge-eval-20260105/\"\n  },\n\n  \"note\": \"9x1 batch testing normalize_weights\"\n}\n```\n\n### Model Spec Modes\n\nFields vary by mode (only include relevant ones):\n\n| Mode | Required Fields |\n|------|-----------------|\n| base | provider, model_id |\n| prompted | provider, model_id, sysprompt_version, sysprompt_sha |\n| trained | provider, base_model, renderer, checkpoint, training_run, training_data, training_config |\n| prompted_trained | all of the above |\n\n### Key Design Decisions\n\n1. **Batch is the unit** - supports NxM (model runs x judge runs) for variance control\n2. **Question-level aggregation** - aggregation field (majority_vote, mean, etc.)\n3. **Partial runs flagged** - complete: false when n_samples \u003c dataset_size, suppressed from queries by default\n4. **Full reproducibility** - capture enough to recreate run exactly (model checkpoint, training config, dataset sha, judge prompt sha)\n5. **Diffing use case** - flat structure, inline small configs, easy to compare two runs\n6. **Version field** - for schema evolution\n7. **Lenient deserialize** - strict on write, permissive on read for forward compat\n\n### Implementation Plan\n\n1. Data layer: Pydantic models with discriminated unions for ModelSpec\n2. CRUD: load/save/query helpers\n3. Tests: thorough coverage of serialization, validation, queries\n4. CLI: isf results list, isf results show, isf results compare\n5. Integration: wire into eval runner to auto-record","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-ri0","title":"Refactor TrainConfig to represent runtime state, not user input","description":"Currently TrainConfig holds user input (with Optional fields for auto-computed values), then the runner mutates it after the fact to fill in resolved values before saving.\n\nThis creates drift risk - the saved config may not match what was actually used if we forget to mutate a field.\n\n## Proposed Fix\n\nTrainConfig should represent the **actual runtime state**, not user input:\n\n1. Runner accepts loose parameters (allowing None for auto-computed)\n2. Runner resolves all values first\n3. Runner builds TrainConfig with all concrete values\n4. TrainConfig fields like `learning_rate`, `renderer`, `shuffle_seed` become required (not Optional)\n\nThis way TrainConfig is always fully specified - it's the record of what happened, enforced by types.\n\n## Changes Needed\n\n- `run_training()` signature takes individual params instead of TrainConfig\n- Runner resolves: renderer, learning_rate, shuffle_seed, name\n- TrainConfig built after resolution with all required fields\n- CLI adapter builds params and calls runner\n- `load_config()` returns a dict or intermediate type, not TrainConfig\n\n## Why This Matters\n\nFor eval results tracking, we embed TrainConfig to capture what training run produced a model. If TrainConfig can have None values, the eval record is incomplete. Tight coupling prevents drift.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-05T13:34:56.043263+02:00","updated_at":"2026-01-05T18:12:30.988424+02:00","closed_at":"2026-01-05T18:12:30.988424+02:00","close_reason":"Refactored TrainConfig to represent fully-resolved runtime state. build_config() now resolves all fields (renderer, learning_rate, shuffle_seed, save_every) before constructing TrainConfig. Runner no longer mutates config."}
{"id":"isf-s60","title":"Add result-vs-result comparison mode","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-05T16:04:28.059506+02:00","updated_at":"2026-01-05T16:11:28.871563+02:00","closed_at":"2026-01-05T16:11:28.871563+02:00","close_reason":"Implemented result-vs-result comparison with short ID resolution","dependencies":[{"issue_id":"isf-s60","depends_on_id":"isf-mlv","type":"depends-on","created_at":"2026-01-05T16:04:51.964208+02:00","created_by":"daemon"}],"comments":[{"id":16,"issue_id":"isf-s60","author":"jlb","text":"## Result vs Result comparison\n\n`isf results compare 0105-6e65 0105-abc1`\n\nCompare specific eval result IDs directly.\n\n### Display format (TBD, initial thoughts):\n\n```\nComparing: 0105-6e65, 0105-abc1\n\nModels:\n  0105-6e65: e037-final (qwen3-30b-a3b, trained)\n  0105-abc1: e038-final (qwen3-30b-a3b, trained)\n\nTraining diffs:\n  learning_rate: 1e-5 vs 2e-5\n\nEval: gpqa-diamond\n  (config matches)\n\nResults:\n  0105-6e65: 59.6% (118/198)\n  0105-abc1: 61.2% (121/198)\n  Δ: +1.6pp\n\nFiles:\n  0105-6e65: results/gpqa-diamond/gpqa-diamond-e037-final-20260105-143022.jsonl\n  0105-abc1: results/gpqa-diamond/gpqa-diamond-e038-final-20260105-152248.jsonl\n```\n\n### Should show:\n- Model training config diffs\n- Eval config diffs (if any fields differ)\n- Detailed results comparison\n- Paths to actual detail files for deeper investigation\n\n### Depends on:\n- Short ID resolution (isf-mlv)\n","created_at":"2026-01-06T07:27:32Z"}]}
{"id":"isf-w49","title":"Clarify registry rebuild command","description":"Consider renaming `isf prompts` to `isf registry` or adding an alias/help text, since the command rebuilds registry.json and not just prompts. Goal: reduce user confusion about where trained/prompted models get registered.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T09:59:38.651462+02:00","updated_at":"2026-01-10T09:59:51.323025+02:00"}
